{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Ray Databricks Connector](images/databrick_connector_logo.png)\n",
    "This user guide walks through the basics of reading and writing data with the Ray and Databricks.\n",
    "\n",
    "The Ray Databricks connector enables parallel read and write to and from a Databricks SQL endpoint. The connector utilizes the Python DB API 2.0 specification implemented by most databases.\n",
    "\n",
    "## Initialize ray\n",
    "Ray will automatically be initialized with defaults when calling any ray or ray dataset methods. To specify configuration, like `loggin_level`, add the below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 23:19:15,409\tINFO worker.py:1242 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "find: ‘.git’: No such file or directory\n",
      "2023-02-21 23:19:15,656\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.17.214:9031...\n",
      "2023-02-21 23:19:15,661\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_vnmb5jgl4z6q98h61dx25rccju/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-02-21 23:19:15,665\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_c63fb84d82dc4174c6a23f011c36ecdf.zip' (0.81MiB) to Ray cluster...\n",
      "2023-02-21 23:19:15,676\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_c63fb84d82dc4174c6a23f011c36ecdf.zip'.\n"
     ]
    }
   ],
   "source": [
    "import ray, logging\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "ray.logger.setLevel(logging.WARN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection properties\n",
    "The databricks  connection properties need to be provided to the data source upon creation. These properties are documented by the databricks.\n",
    "\n",
    "Below is an example of loading properties from the environment, and filtering them by the 'DATABRICKS_' prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection properties:\n",
      "catalog\n",
      "schema\n",
      "server_hostname\n",
      "access_token\n",
      "http_path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "env_connect_props = {\n",
    "    key.replace('DATABRICKS_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'DATABRICKS_' in key\n",
    "}\n",
    "\n",
    "env_connect_props = dict(\n",
    "    server_hostname='dbc-fa3925fd-9102.cloud.databricks.com',\n",
    "    access_token='10fda01a-72f1-41c7-a397-90b7b09d6679',\n",
    "    http_path='/sql/1.0/warehouses/db3be239af982b71'\n",
    ")\n",
    "\n",
    "# add db and schema in connect props\n",
    "# add db and schema in connect props\n",
    "connect_props = dict(\n",
    "    catalog = 'samples',\n",
    "    schema = 'tpch',\n",
    "    **env_connect_props\n",
    ")\n",
    "\n",
    "print('Connection properties:')\n",
    "print('\\n'.join(connect_props.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading\n",
    "Ray will use Databricks Python API to read in parallel into a Ray cluster. The created Ray datasets is composed of PyArrow dataframes that are spread across the Ray cluster to allow for the distributed operations required in machine learning.\n",
    "\n",
    "![Databricks Read](images/databricks_read.png)\n",
    "\n",
    "### Read from tables\n",
    "In order to read an entire table into a a Ray cluster, utilize the Ray data `read_databricks` method. The code below will read in a sample table from a Databricks sample database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=226600) opening database connection\n",
      "(_get_read_tasks pid=226600) executing on database:\n",
      "(_get_read_tasks pid=226600)   'SELECT COUNT(*) FROM (customer)'\n",
      "(_get_read_tasks pid=226600) committing database transaction\n",
      "(_get_read_tasks pid=226600) closing database connection\n",
      "(_get_read_tasks pid=226600) opening database connection\n",
      "(_get_read_tasks pid=226600) executing on database:\n",
      "(_get_read_tasks pid=226600)   'SELECT * FROM (customer) LIMIT 100'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=226600) committing database transaction\n",
      "(_get_read_tasks pid=226600) closing database connection\n",
      "(_execute_read_task_split pid=226600) opening database connection\n",
      "(_execute_read_task_split pid=226600) executing on database:\n",
      "(_execute_read_task_split pid=226600)   'SELECT * FROM (customer) LIMIT 5515 OFFSET 0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 1376.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_custkey</th>\n",
       "      <th>c_name</th>\n",
       "      <th>c_address</th>\n",
       "      <th>c_nationkey</th>\n",
       "      <th>c_phone</th>\n",
       "      <th>c_acctbal</th>\n",
       "      <th>c_mktsegment</th>\n",
       "      <th>c_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>412445</td>\n",
       "      <td>Customer#000412445</td>\n",
       "      <td>0QAB3OjYnbP6mA0B,kgf</td>\n",
       "      <td>21</td>\n",
       "      <td>31-421-403-4333</td>\n",
       "      <td>5358.33</td>\n",
       "      <td>BUILDING</td>\n",
       "      <td>arefully blithely regular epi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>412446</td>\n",
       "      <td>Customer#000412446</td>\n",
       "      <td>5u8MSbyiC7J,7PuY4Ivaq1JRbTCMKeNVqg</td>\n",
       "      <td>20</td>\n",
       "      <td>30-487-949-7942</td>\n",
       "      <td>9441.59</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>sleep according to the fluffily even forges. f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>412447</td>\n",
       "      <td>Customer#000412447</td>\n",
       "      <td>HC4ZT62gKPgrjr ceoaZgFOunlUogr7GO</td>\n",
       "      <td>7</td>\n",
       "      <td>17-797-466-6308</td>\n",
       "      <td>7868.75</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>aggle blithely among the carefully express excus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c_custkey              c_name                            c_address  \\\n",
       "0     412445  Customer#000412445                 0QAB3OjYnbP6mA0B,kgf   \n",
       "1     412446  Customer#000412446  5u8MSbyiC7J,7PuY4Ivaq1JRbTCMKeNVqg    \n",
       "2     412447  Customer#000412447    HC4ZT62gKPgrjr ceoaZgFOunlUogr7GO   \n",
       "\n",
       "   c_nationkey          c_phone c_acctbal c_mktsegment  \\\n",
       "0           21  31-421-403-4333   5358.33     BUILDING   \n",
       "1           20  30-487-949-7942   9441.59    MACHINERY   \n",
       "2            7  17-797-466-6308   7868.75   AUTOMOBILE   \n",
       "\n",
       "                                           c_comment  \n",
       "0                      arefully blithely regular epi  \n",
       "1  sleep according to the fluffily even forges. f...  \n",
       "2   aggle blithely among the carefully express excus  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data import read_databricks\n",
    "\n",
    "# read the table, limiting to first 1K customers\n",
    "ds = read_databricks(connect_props, table='customer').limit(1000)\n",
    "\n",
    "# display the first 3 results\n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read with a query\n",
    "For more control over columns and rows read, as well as joining data from multiple tables, a query can be specified instead of a table name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_execute_read_task_split pid=226600) committing database transaction\n",
      "(_execute_read_task_split pid=226600) closing database connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=226600) opening database connection\n",
      "(_get_read_tasks pid=226600) executing on database:\n",
      "(_get_read_tasks pid=226600)   'SELECT COUNT(*) FROM (SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL < 0 LIMIT 1000)'\n",
      "(_get_read_tasks pid=226600) committing database transaction\n",
      "(_get_read_tasks pid=226600) closing database connection\n",
      "(_get_read_tasks pid=226600) opening database connection\n",
      "(_get_read_tasks pid=226600) executing on database:\n",
      "(_get_read_tasks pid=226600)   'SELECT * FROM (SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL < 0 LIMIT 1000) LIMIT 100'\n",
      "(_get_read_tasks pid=226600)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " committing database transaction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=226600) closing database connection\n",
      "(_execute_read_task_split pid=226600) opening database connection\n",
      "(_execute_read_task_split pid=226600) executing on database:\n",
      "(_execute_read_task_split pid=226600)   'SELECT * FROM (SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL < 0 LIMIT 1000) LIMIT 72 OFFSET 0'\n",
      "(_execute_read_task_split pid=226600) committing database transaction\n",
      "(_execute_read_task_split pid=226600) closing database connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 1173.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_ACCTBAL</th>\n",
       "      <th>C_MKTSEGMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-219.53</td>\n",
       "      <td>BUILDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-778.23</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-848.16</td>\n",
       "      <td>BUILDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  C_ACCTBAL C_MKTSEGMENT\n",
       "0   -219.53     BUILDING\n",
       "1   -778.23   AUTOMOBILE\n",
       "2   -848.16     BUILDING"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = 'SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL < 0 LIMIT 1000'\n",
    "\n",
    "# read the result of the query\n",
    "ds2 = read_databricks(connect_props, query=QUERY)\n",
    "\n",
    "# display the first 3 results\n",
    "ds2.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional read parameters\n",
    "For reading from Databricks, underlying Python API arguments are also available and can be passed to the underlying execute method.\n",
    "\n",
    "The code below uses the parameters argument to specify parameterss to be used by Databricks when executing the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=226600) opening database connection\n",
      "(_get_read_tasks pid=226600) executing on database:\n",
      "(_get_read_tasks pid=226600)   'SELECT COUNT(*) FROM (SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL > %(balance)i LIMIT 100)'\n",
      "(_get_read_tasks pid=226600)   parameters=dict(balance=2.0)\n",
      "(_get_read_tasks pid=226600) committing database transaction\n",
      "(_get_read_tasks pid=226600) closing database connection\n",
      "(_get_read_tasks pid=226600) opening database connection\n",
      "(_get_read_tasks pid=226600) executing on database:\n",
      "(_get_read_tasks pid=226600)   'SELECT * FROM (SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL > %(balance)i LIMIT 100) LIMIT 100'\n",
      "(_get_read_tasks pid=226600)   parameters=dict(balance=2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=226600) committing database transaction\n",
      "(_get_read_tasks pid=226600) closing database connection\n",
      "(_execute_read_task_split pid=226600) opening database connection\n",
      "(_execute_read_task_split pid=226600) executing on database:\n",
      "(_execute_read_task_split pid=226600)   'SELECT * FROM (SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL > %(balance)i LIMIT 100) LIMIT 8 OFFSET 0'\n",
      "(_execute_read_task_split pid=226600)   parameters=dict(balance=2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_execute_read_task_split pid=226600) committing database transaction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_execute_read_task_split pid=226600) closing database connection"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 469.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_ACCTBAL</th>\n",
       "      <th>C_MKTSEGMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5358.33</td>\n",
       "      <td>BUILDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9441.59</td>\n",
       "      <td>MACHINERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7868.75</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  C_ACCTBAL C_MKTSEGMENT\n",
       "0   5358.33     BUILDING\n",
       "1   9441.59    MACHINERY\n",
       "2   7868.75   AUTOMOBILE"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = 'SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL > %(balance)i LIMIT 100'\n",
    "\n",
    "ds3 = read_databricks(connect_props, query=QUERY, parameters={'balance':2.0})\n",
    "ds3.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing\n",
    "The Ray Databricks connector will use the Databricks driver to write each partition of data in parallel. Each partition of data in the Ray dataset will have a write task that writes in parallel to a cload strorage location. After the \n",
    "partitions are written, a table is created using the parquet files. This \n",
    "![Databricks write](images/databricks_write.png)\n",
    "\n",
    "### Write to tables\n",
    "In order to write a dataset into database table, use the `write_dbapi2` method of the dataset object. Repartition the dataset prior to calling this method in order to set the number of write tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sql import connect\n",
    "\n",
    "write_connect_props = {\n",
    "    **connect_props, \n",
    "    'catalog':'hive_metastore',\n",
    "    'schema':'default'\n",
    "}\n",
    "\n",
    "# create destination table\n",
    "with connect(**write_connect_props) as con:\n",
    "    with con.cursor() as cursor:\n",
    "        cursor.execute('DROP TABLE IF EXISTS customer2')\n",
    "        #cursor.execute('CREATE TABLE customer2 USING DELTA AS (SELECT * FROM samples.tpch.customer LIMIT 0)')\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below writes the previously read data into a new database table that are created using the Snowflake Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 23:21:46,031\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[write]\n",
      "write:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_map_task pid=226600) staging data to s3://egr-sydney-databricks/stage/7fb6e362-de66-4023-a1b8-77fe7aa4f9f4\n",
      "(_map_task pid=226600) writing db blocks\n",
      "(_map_task pid=226600) opening database connection\n",
      "(_map_task pid=226600) adding writer for partition 0\n",
      "(_map_task pid=226600) CREATE TABLE IF NOT EXISTS customer2\n",
      "(_map_task pid=226600) executing on database:\n",
      "(_map_task pid=226600)   'CREATE TABLE IF NOT EXISTS customer2'\n",
      "(_map_task pid=226600) committing database transaction\n",
      "(_map_task pid=226600) closing database connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening database connection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 23:21:47,145\tINFO client.py:193 -- Successfully opened session b'\\x01\\xed\\xb2\\x81\\x90\\xfb\\x1c\\xe0\\xbc\\xe8B{p\\x96\\xac\\xd6'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing on database:\n",
      "  'COPY INTO customer2 \n",
      "  FROM 's3://egr-sydney-databricks/stage/7fb6e362-de66-4023-a1b8-77fe7aa4f9f4/'\n",
      "   WITH ( CREDENTIAL `8b246fc0-6157-42de-8d3e-21fc2dff232b-data-access-config-1676359658624` )     \n",
      "  FILEFORMAT = PARQUET\n",
      "  PATTERN = '*.parquet'\n",
      "  FORMAT_OPTIONS (\n",
      "      'mergeSchema' = 'true'\n",
      "  )\n",
      "  COPY_OPTIONS ('mergeSchema' = 'true')\n",
      "  '\n",
      "committing database transaction\n",
      "committing database transaction\n",
      "closing database connection\n",
      "(_get_read_tasks pid=226600) opening database connection\n",
      "(_get_read_tasks pid=226600) executing on database:\n",
      "(_get_read_tasks pid=226600)   'SELECT COUNT(*) FROM (hive_metastore.default.customer2)'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=226600) 2023-02-21 23:21:53,696\tERROR database_datasource.py:94 -- error executing :"
     ]
    },
    {
     "ename": "RayTaskError(ServerOperationError)",
     "evalue": "\u001b[36mray::_get_read_tasks()\u001b[39m (pid=226600, ip=10.0.17.214)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/read_api.py\", line 1795, in _get_read_tasks\n    requested_parallelism, min_safe_parallelism = _autodetect_parallelism(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/util.py\", line 113, in _autodetect_parallelism\n    mem_size = reader.estimate_inmemory_data_size()\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 376, in estimate_inmemory_data_size\n    if self.num_rows and self.sample is not None:\n  File \"/home/ray/anaconda3/lib/python3.10/functools.py\", line 981, in __get__\n    val = self.func(instance)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 360, in num_rows\n    num_rows = self.connector.query_int(query, **self.query_kwargs)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 263, in query_int\n    value = self.query_value(query, **kwargs)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 260, in query_value\n    return self.to_value_fn(self.execute(query, **kwargs))\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 289, in execute\n    raise e\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 283, in execute\n    return self._execute(query, **kwargs)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/dbapi2_datasource.py\", line 82, in _execute\n    cursor.execute(q, *query_args, **query_kwargs)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/client.py\", line 472, in execute\n    execute_response = self.thrift_backend.execute_command(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 833, in execute_command\n    return self._handle_execute_response(resp, cursor)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 925, in _handle_execute_response\n    final_operation_state = self._wait_until_command_done(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 767, in _wait_until_command_done\n    self._check_command_not_in_error_or_closed_state(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 484, in _check_command_not_in_error_or_closed_state\n    raise ServerOperationError(\ndatabricks.sql.exc.ServerOperationError: You are trying to read a table `hive_metastore`.`default`.`customer2` without columns using Delta.\n\nWrite some data with option `mergeSchema = true` to enable subsequent read access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(ServerOperationError)\u001b[0m        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m ds\u001b[39m.\u001b[39mwrite_databricks(\n\u001b[1;32m      3\u001b[0m     write_connect_props, \n\u001b[1;32m      4\u001b[0m     table\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcustomer2\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     credential\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m8b246fc0-6157-42de-8d3e-21fc2dff232b-data-access-config-1676359658624\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     stage_uri\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://egr-sydney-databricks/stage\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[39m# read the new table\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m ds4 \u001b[39m=\u001b[39m read_databricks(write_connect_props, table\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhive_metastore.default.customer2\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m ds4\u001b[39m.\u001b[39mlimit(\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/read_api.py:469\u001b[0m, in \u001b[0;36mread_databricks\u001b[0;34m(connect_properties, table, query, mode, parallelism, ray_remote_args, **databricks_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m connector \u001b[39m=\u001b[39m DatabricksConnector(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconnect_properties)\n\u001b[1;32m    468\u001b[0m datasource \u001b[39m=\u001b[39m DatabricksDatasource(connector)\n\u001b[0;32m--> 469\u001b[0m \u001b[39mreturn\u001b[39;00m read_datasource(\n\u001b[1;32m    470\u001b[0m     datasource,\n\u001b[1;32m    471\u001b[0m     table\u001b[39m=\u001b[39;49mtable,\n\u001b[1;32m    472\u001b[0m     query\u001b[39m=\u001b[39;49mquery,\n\u001b[1;32m    473\u001b[0m     mode \u001b[39m=\u001b[39;49m mode,\n\u001b[1;32m    474\u001b[0m     parallelism \u001b[39m=\u001b[39;49m parallelism,\n\u001b[1;32m    475\u001b[0m     ray_remote_args\u001b[39m=\u001b[39;49mray_remote_args,\n\u001b[1;32m    476\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdatabricks_kwargs\n\u001b[1;32m    477\u001b[0m )\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/read_api.py:307\u001b[0m, in \u001b[0;36mread_datasource\u001b[0;34m(datasource, parallelism, ray_remote_args, **read_args)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[39m# Prepare read in a remote task so that in Ray client mode, we aren't\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39m# attempting metadata resolution from the client machine.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     get_read_tasks \u001b[39m=\u001b[39m cached_remote_fn(\n\u001b[1;32m    304\u001b[0m         _get_read_tasks, retry_exceptions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_cpus\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m     requested_parallelism, min_safe_parallelism, read_tasks \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(\n\u001b[1;32m    308\u001b[0m         get_read_tasks\u001b[39m.\u001b[39;49mremote(\n\u001b[1;32m    309\u001b[0m             datasource,\n\u001b[1;32m    310\u001b[0m             ctx,\n\u001b[1;32m    311\u001b[0m             cur_pg,\n\u001b[1;32m    312\u001b[0m             parallelism,\n\u001b[1;32m    313\u001b[0m             local_uri,\n\u001b[1;32m    314\u001b[0m             _wrap_arrow_serialization_workaround(read_args),\n\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m read_tasks \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(read_tasks) \u001b[39m<\u001b[39m min_safe_parallelism \u001b[39m*\u001b[39m \u001b[39m0.7\u001b[39m:\n\u001b[1;32m    319\u001b[0m     perc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mround\u001b[39m((min_safe_parallelism \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(read_tasks)) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(read_tasks), \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/_private/worker.py:2384\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2382\u001b[0m     worker\u001b[39m.\u001b[39mcore_worker\u001b[39m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2384\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2385\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2386\u001b[0m     \u001b[39mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(ServerOperationError)\u001b[0m: \u001b[36mray::_get_read_tasks()\u001b[39m (pid=226600, ip=10.0.17.214)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/read_api.py\", line 1795, in _get_read_tasks\n    requested_parallelism, min_safe_parallelism = _autodetect_parallelism(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/util.py\", line 113, in _autodetect_parallelism\n    mem_size = reader.estimate_inmemory_data_size()\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 376, in estimate_inmemory_data_size\n    if self.num_rows and self.sample is not None:\n  File \"/home/ray/anaconda3/lib/python3.10/functools.py\", line 981, in __get__\n    val = self.func(instance)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 360, in num_rows\n    num_rows = self.connector.query_int(query, **self.query_kwargs)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 263, in query_int\n    value = self.query_value(query, **kwargs)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 260, in query_value\n    return self.to_value_fn(self.execute(query, **kwargs))\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 289, in execute\n    raise e\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 283, in execute\n    return self._execute(query, **kwargs)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/dbapi2_datasource.py\", line 82, in _execute\n    cursor.execute(q, *query_args, **query_kwargs)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/client.py\", line 472, in execute\n    execute_response = self.thrift_backend.execute_command(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 833, in execute_command\n    return self._handle_execute_response(resp, cursor)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 925, in _handle_execute_response\n    final_operation_state = self._wait_until_command_done(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 767, in _wait_until_command_done\n    self._check_command_not_in_error_or_closed_state(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 484, in _check_command_not_in_error_or_closed_state\n    raise ServerOperationError(\ndatabricks.sql.exc.ServerOperationError: You are trying to read a table `hive_metastore`.`default`.`customer2` without columns using Delta.\n\nWrite some data with option `mergeSchema = true` to enable subsequent read access."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(_get_read_tasks pid=226600)   'SELECT COUNT(*) FROM (hive_metastore.default.customer2)'\n",
      "(_get_read_tasks pid=226600) Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=226600)   File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 283, in execute\n",
      "(_get_read_tasks pid=226600)     return self._execute(query, **kwargs)\n",
      "(_get_read_tasks pid=226600)   File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/dbapi2_datasource.py\", line 82, in _execute\n",
      "(_get_read_tasks pid=226600)     cursor.execute(q, *query_args, **query_kwargs)\n",
      "(_get_read_tasks pid=226600)   File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/client.py\", line 472, in execute\n",
      "(_get_read_tasks pid=226600)     execute_response = self.thrift_backend.execute_command(\n",
      "(_get_read_tasks pid=226600)   File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 833, in execute_command\n",
      "(_get_read_tasks pid=226600)     return self._handle_execute_response(resp, cursor)\n",
      "(_get_read_tasks pid=226600)   File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 925, in _handle_execute_response\n",
      "(_get_read_tasks pid=226600)     final_operation_state = self._wait_until_command_done(\n",
      "(_get_read_tasks pid=226600)   File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 767, in _wait_until_command_done\n",
      "(_get_read_tasks pid=226600)     self._check_command_not_in_error_or_closed_state(\n",
      "(_get_read_tasks pid=226600)   File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/databricks/sql/thrift_backend.py\", line 484, in _check_command_not_in_error_or_closed_state\n",
      "(_get_read_tasks pid=226600)     raise ServerOperationError(\n",
      "(_get_read_tasks pid=226600) databricks.sql.exc.ServerOperationError: You are trying to read a table `hive_metastore`.`default`.`customer2` without columns using Delta.\n",
      "(_get_read_tasks pid=226600) \n",
      "(_get_read_tasks pid=226600) Write some data with option `mergeSchema = true` to enable subsequent read access.\n",
      "(_get_read_tasks pid=226600) 2023-02-21 23:21:53,703\tWARNING database_datasource.py:96 -- databricks does not support rollback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=226600) error executing :\n",
      "(_get_read_tasks pid=226600)   'SELECT COUNT(*) FROM (hive_metastore.default.customer2)'\n",
      "(_get_read_tasks pid=226600) rolling back database transaction\n",
      "(_get_read_tasks pid=226600) databricks does not support rollback.\n",
      "(_get_read_tasks pid=226600) committing database transaction\n",
      "(_get_read_tasks pid=226600) closing database connection\n"
     ]
    }
   ],
   "source": [
    "# write the dataset to the table \n",
    "ds.write_databricks(\n",
    "    write_connect_props, \n",
    "    table='hive_metastore.default.customer2',\n",
    "    credential='8b246fc0-6157-42de-8d3e-21fc2dff232b-data-access-config-1676359658624',\n",
    "    stage_uri='s3://egr-sydney-databricks/stage'\n",
    ")\n",
    "\n",
    "# read the new table\n",
    "ds4 = read_databricks(write_connect_props, table='customer2')\n",
    "ds4.limit(3).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "If more low level access to the Ray Databricks connector is needed, the underlying `DatabricksConnector` and `DatabricksDatasource` can be used.\n",
    "\n",
    "### Databricks Connector\n",
    "The `DatabricksConnector` class holds the connection properties and logic required to establish a connection with Databricks. Internally it calls the native Python dirver API in order to read and write from and to tables in parallel across the cluster. The datasource uses the DB API 2 `execute` and `executemany` methods to enable parallel read and writes of data.\n",
    "\n",
    "The connector is also a Python context manager, and utilize `with` semantics to define when a connection should be established, db operations commited to the database, and the connection closed. \n",
    "\n",
    "The code below will read from a sample table using the connector to manage the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import DatabricksConnector\n",
    "\n",
    "# query the number of rows, using the connection context to\n",
    "# manage transactions\n",
    "with DatabricksConnector(**connect_props) as connector:\n",
    "    count = connector.query_int(f'SELECT COUNT(*) FROM customer')\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use `try` blocks with the connector's `open`, `commit` and `close` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connector = DatabricksConnector(**write_connect_props)\n",
    "    connector.open()\n",
    "    count = connector.query_int(f'SELECT COUNT(*) FROM customer2')\n",
    "finally:\n",
    "    connector.close()\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databricks Datasource\n",
    "The Databricks datasource can be used with the Ray data `read_datasource` and `write_datasource` methods to read and write to databases using the distibuted processing capabilities of Ray data. The datasource uses the DatabricksConnector class internally.\n",
    "\n",
    "Below is an example of creating the datasource using the previously defined connect properties, and then using it to read and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import DatabricksDatasource\n",
    "from ray.data import read_datasource\n",
    "\n",
    "# create a datasource from a connector\n",
    "datasource = DatabricksDatasource(connector)\n",
    "\n",
    "# use read_datasource to read\n",
    "ds = read_datasource(datasource, table='customer2')\n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DML and DDL\n",
    "The connector can also be used for any DDL or DML operations you would normally execute through the DB Native Python API. These operations just pass through to the underlying API. \n",
    "\n",
    "The code below will create the objects needed for writing to tables. Note that a commit is issued between the queries so the DDL operation executes prior to the next one that is dependent. An alternative is to use two `with` blocks to define transaction boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connector as con:\n",
    "    con.query('DROP TABLE IF EXISTS customer2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
