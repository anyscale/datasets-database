{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Snowflake connector](images/ray_snowflake.png)\n",
    "This example walks through the basics of reading and writing data with the Ray Snowflake connector.\n",
    "\n",
    "## Connection properties\n",
    "The Snowflake connection properties need to be provided to the data source upon creation. The minimal required properties are `user`, `password`, `account` and `warehouse`. To use API keys instead of a password, functionality to load Snowflake API keys is also provided. API keys can be loaded from a file specified by the `private_key_file` property, or can be passed directly via the `private_key` property. If the key is password protected, the password can be given via the `pk_password` property.  Optional properties like database and schema can also be provided at construction or be included in the fully specified table name of format `db.schema.table` when calling read or write operations with a table or subquery.\n",
    "\n",
    "Below is an example of loading properties from the environment, and filtering them by the 'SNOWFLAKE_' prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection properties:\n",
      "database\n",
      "schema\n",
      "warehouse\n",
      "account\n",
      "private_key_file\n",
      "pk_password\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get properties form env\n",
    "env_connect_props = {\n",
    "    key.replace('SNOWFLAKE_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'SNOWFLAKE_' in key\n",
    "}\n",
    "\n",
    "# add db and schema in connect props\n",
    "connect_props = dict(\n",
    "    database = 'SNOWFLAKE_SAMPLE_DATA',\n",
    "    schema = 'TPCH_SF1',\n",
    "    warehouse='COMPUTE_WH',\n",
    "    **env_connect_props\n",
    ")\n",
    "\n",
    "print('Connection properties:')\n",
    "print('\\n'.join(connect_props.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from Snowflake\n",
    "Ray will use Snowflake optimizations that allow query results to be read in parallel into a Ray cluster. The created Ray datasets is composed of Pandas dataframes that are spread across the Ray cluster to allow for the distributed operations required in machine learning.\n",
    "\n",
    "![Snowflake read table](images/snowflake_read_table.png)\n",
    "\n",
    "\n",
    "### Read from tables\n",
    "In order to read an entire table into a a Ray cluster, utilize the Ray data `read_snowflake` method. The code below will read in a sample table from the Snowflake sample database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 19:23:27,189\tINFO worker.py:1230 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "find: ‘.git’: No such file or directory\n",
      "2023-02-07 19:23:27,548\tINFO worker.py:1352 -- Connecting to existing Ray cluster at address: 10.0.59.103:9031...\n",
      "2023-02-07 19:23:27,557\tINFO worker.py:1529 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_vnmb5jgl4z6q98h61dx25rccju/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-02-07 19:23:27,564\tINFO packaging.py:373 -- Pushing file package 'gcs://_ray_pkg_f2af749f7a8069559882c5287bc7bb72.zip' (0.71MiB) to Ray cluster...\n",
      "2023-02-07 19:23:27,578\tINFO packaging.py:386 -- Successfully pushed file package 'gcs://_ray_pkg_f2af749f7a8069559882c5287bc7bb72.zip'.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 88.30it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 930.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_CUSTKEY</th>\n",
       "      <th>C_NAME</th>\n",
       "      <th>C_ADDRESS</th>\n",
       "      <th>C_NATIONKEY</th>\n",
       "      <th>C_PHONE</th>\n",
       "      <th>C_ACCTBAL</th>\n",
       "      <th>C_MKTSEGMENT</th>\n",
       "      <th>C_COMMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30001</td>\n",
       "      <td>Customer#000030001</td>\n",
       "      <td>Ui1b,3Q71CiLTJn4MbVp,,YCZARIaNTelfst</td>\n",
       "      <td>4</td>\n",
       "      <td>14-526-204-4500</td>\n",
       "      <td>8848.47</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>frays wake blithely enticingly ironic asymptote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30002</td>\n",
       "      <td>Customer#000030002</td>\n",
       "      <td>UVBoMtILkQu1J3v</td>\n",
       "      <td>11</td>\n",
       "      <td>21-340-653-9800</td>\n",
       "      <td>5221.81</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>he slyly ironic pinto beans wake slyly above t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30003</td>\n",
       "      <td>Customer#000030003</td>\n",
       "      <td>CuGi9fwKn8JdR</td>\n",
       "      <td>21</td>\n",
       "      <td>31-757-493-7525</td>\n",
       "      <td>3014.89</td>\n",
       "      <td>BUILDING</td>\n",
       "      <td>e furiously alongside of the requests. evenly ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_CUSTKEY              C_NAME                             C_ADDRESS  \\\n",
       "0      30001  Customer#000030001  Ui1b,3Q71CiLTJn4MbVp,,YCZARIaNTelfst   \n",
       "1      30002  Customer#000030002                       UVBoMtILkQu1J3v   \n",
       "2      30003  Customer#000030003                         CuGi9fwKn8JdR   \n",
       "\n",
       "   C_NATIONKEY          C_PHONE  C_ACCTBAL C_MKTSEGMENT  \\\n",
       "0            4  14-526-204-4500    8848.47    MACHINERY   \n",
       "1           11  21-340-653-9800    5221.81    MACHINERY   \n",
       "2           21  31-757-493-7525    3014.89     BUILDING   \n",
       "\n",
       "                                           C_COMMENT  \n",
       "0    frays wake blithely enticingly ironic asymptote  \n",
       "1  he slyly ironic pinto beans wake slyly above t...  \n",
       "2  e furiously alongside of the requests. evenly ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data import read_snowflake\n",
    "\n",
    "# read the entire table\n",
    "ds = read_snowflake(connect_props, table='CUSTOMER') \n",
    "\n",
    "# display the first 3 results\n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read with a query\n",
    "For more control over columns and rows read, as well as joining data from multiple tables, a query can be specified instead of a table name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 19:23:35,439\tWARNING read_api.py:332 -- ⚠️  The number of blocks in this dataset (2) limits its parallelism to 2 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 190.02it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 1197.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_ACCTBAL</th>\n",
       "      <th>C_MKTSEGMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-278.54</td>\n",
       "      <td>MACHINERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-970.23</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-554.23</td>\n",
       "      <td>FURNITURE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_ACCTBAL C_MKTSEGMENT\n",
       "0    -278.54    MACHINERY\n",
       "1    -970.23    HOUSEHOLD\n",
       "2    -554.23    FURNITURE"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = 'SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL < 0'\n",
    "\n",
    "# read the result of the query\n",
    "ds2 = read_snowflake(connect_props, query=QUERY)\n",
    "\n",
    "# display the first 3 results\n",
    "ds2.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional read parameters\n",
    "For reading from Snowflake, underlying Python API arguments are also available. The `timeout` and `params` arguments may be used in the [cursor execute method](https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor).\n",
    "\n",
    "The code below uses the params to specify params to be used by Snowflake when executing the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 132.56it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 947.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_ACCTBAL</th>\n",
       "      <th>C_MKTSEGMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9957.56</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2526.92</td>\n",
       "      <td>BUILDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7975.22</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_ACCTBAL C_MKTSEGMENT\n",
       "0    9957.56    HOUSEHOLD\n",
       "1    2526.92     BUILDING\n",
       "2    7975.22   AUTOMOBILE"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = 'SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL > ?'\n",
    "\n",
    "ds3 = read_snowflake(connect_props, query=QUERY, params=[1000], timeout=1000)\n",
    "ds3.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing\n",
    "The Ray Snowflake connector will use Snowflake API to write each partition of data in parallel. Each partition of data in the Ray dataset will have a write task that writes in parallel to Snowflake. \n",
    "![Snowflake write table](images/snowflake_write_table.png)\n",
    "\n",
    "### Write to tables\n",
    "In order to write a dataset into Snowflake table, use the `write_snowflake` method of the dataset object. Repartition the dataset in order to set the number of write tasks.\n",
    "\n",
    "First, a new database and table needs to be created using the Ray Snowflake Connector or the native Snowflake API. \n",
    "\n",
    "> Note: When using the Ray data connector, we can use the API key loading functionality built into the connector class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import SnowflakeConnector\n",
    "\n",
    "write_connect_props = {\n",
    "    **connect_props, \n",
    "    'database':'RAY_SAMPLE', \n",
    "    'schema':'PUBLIC'\n",
    "}\n",
    "with SnowflakeConnector(**write_connect_props) as con:\n",
    "    # create destination database\n",
    "    con.query(f'CREATE DATABASE IF NOT EXISTS RAY_SAMPLE')\n",
    "    con.commit()\n",
    "    \n",
    "    # create destination table\n",
    "    con.query('''\n",
    "        CREATE OR REPLACE TABLE CUSTOMER_COPY \n",
    "        LIKE SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n",
    "    ''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below writes the previously read data into a new database table that are created using the Snowflake Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 19/19 [00:03<00:00,  5.80it/s]\n"
     ]
    },
    {
     "ename": "RayTaskError(TypeError)",
     "evalue": "\u001b[36mray::_do_write()\u001b[39m (pid=24431, ip=10.0.59.103)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py\", line 4482, in _do_write\n    return ds.do_write(blocks, meta, ray_remote_args=ray_remote_args, **write_args)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 414, in do_write\n    columns = _get_columns(metadata)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 36, in _get_columns\n    if type(metadata.schema, dict):\nTypeError: type() takes 1 or 3 arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# write the dataset to the table \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ds\u001b[39m.\u001b[39;49mwrite_snowflake(\n\u001b[1;32m      3\u001b[0m     write_connect_props, \n\u001b[1;32m      4\u001b[0m     table\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mCUSTOMER_COPY\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py:2612\u001b[0m, in \u001b[0;36mDataset.write_snowflake\u001b[0;34m(self, connect_properties, table, ray_remote_args, **databrick_args)\u001b[0m\n\u001b[1;32m   2610\u001b[0m connector \u001b[39m=\u001b[39m SnowflakeConnector(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconnect_properties)\n\u001b[1;32m   2611\u001b[0m datasource \u001b[39m=\u001b[39m SnowflakeDatasource(connector)\n\u001b[0;32m-> 2612\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_datasource(\n\u001b[1;32m   2613\u001b[0m     datasource,\n\u001b[1;32m   2614\u001b[0m     table\u001b[39m=\u001b[39;49mtable,\n\u001b[1;32m   2615\u001b[0m     ray_remote_args\u001b[39m=\u001b[39;49mray_remote_args,\n\u001b[1;32m   2616\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdatabrick_args\n\u001b[1;32m   2617\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py:2728\u001b[0m, in \u001b[0;36mDataset.write_datasource\u001b[0;34m(self, datasource, ray_remote_args, **write_args)\u001b[0m\n\u001b[1;32m   2724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2725\u001b[0m     \u001b[39m# Prepare write in a remote task so that in Ray client mode, we\u001b[39;00m\n\u001b[1;32m   2726\u001b[0m     \u001b[39m# don't do metadata resolution from the client machine.\u001b[39;00m\n\u001b[1;32m   2727\u001b[0m     do_write \u001b[39m=\u001b[39m cached_remote_fn(_do_write, retry_exceptions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_cpus\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m-> 2728\u001b[0m     write_results: List[ObjectRef[WriteResult]] \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(\n\u001b[1;32m   2729\u001b[0m         do_write\u001b[39m.\u001b[39;49mremote(\n\u001b[1;32m   2730\u001b[0m             datasource,\n\u001b[1;32m   2731\u001b[0m             ctx,\n\u001b[1;32m   2732\u001b[0m             blocks,\n\u001b[1;32m   2733\u001b[0m             metadata,\n\u001b[1;32m   2734\u001b[0m             ray_remote_args,\n\u001b[1;32m   2735\u001b[0m             _wrap_arrow_serialization_workaround(write_args),\n\u001b[1;32m   2736\u001b[0m         )\n\u001b[1;32m   2737\u001b[0m     )\n\u001b[1;32m   2739\u001b[0m progress \u001b[39m=\u001b[39m ProgressBar(\u001b[39m\"\u001b[39m\u001b[39mWrite Progress\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(write_results))\n\u001b[1;32m   2740\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ray/_private/worker.py:2309\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2307\u001b[0m     worker\u001b[39m.\u001b[39mcore_worker\u001b[39m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2308\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2309\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2311\u001b[0m     \u001b[39mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m: \u001b[36mray::_do_write()\u001b[39m (pid=24431, ip=10.0.59.103)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/dataset.py\", line 4482, in _do_write\n    return ds.do_write(blocks, meta, ray_remote_args=ray_remote_args, **write_args)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 414, in do_write\n    columns = _get_columns(metadata)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 36, in _get_columns\n    if type(metadata.schema, dict):\nTypeError: type() takes 1 or 3 arguments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(scheduler +7m34s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "(scheduler +7m34s) Resized to 23 CPUs.\n",
      "(scheduler +12m40s) Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "(scheduler +12m56s) Resized to 7 CPUs.\n",
      "(scheduler +26m49s) Adding 1 node(s) of type worker-node-type-0.\n",
      "(scheduler +27m38s) Resized to 23 CPUs.\n",
      "(scheduler +32m43s) Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "(scheduler +32m58s) Resized to 7 CPUs.\n"
     ]
    }
   ],
   "source": [
    "# write the dataset to the table \n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='CUSTOMER_COPY'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional write parameters\n",
    "For writing to Snowflake, the native Snowflake API arguments are also available from the [write_pandas](https://docs.snowflake.com/en/user-guide/python-connector-api.html#module-snowflake-connector-pandas-tools) method. The following is a list of the parameters that may be useful:\n",
    "\n",
    "- `auto_create_table`: When true, will automatically create a table with corresponding columns for each column in the passed in DataFrame. The table will not be created if it already exists\n",
    "- `overwrite`: When true, and if auto_create_table is true, then it drops the table. Otherwise, it truncates the table. In both cases it will replace the existing contents of the table with that of the passed in Pandas DataFrame.\n",
    "- `table_type`: The table type of to-be-created table. The supported table types include ``temp``/``temporary`` and ``transient``. Empty means permanent table as per SQL convention.\n",
    "\n",
    "In the example below, we use the `auto_create_table` parameter to create the output table before writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataset to the table, using an autocreated table\n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='CUSTOMER_COPY_2',\n",
    "    auto_create_table=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "If more low level access to the Ray Snowflake connector is needed, the underlying `SnowflakConnector` and `SnowflakeDatasource` can be used.\n",
    "\n",
    "### Snowflake Connector\n",
    "The `SnowflakeConnector` class holds the connection properties and logic required to establish a connection with Snowflake. Internally it calls the native Python Snowflake API in order to read and write from and to Snowflake tables in parallel across the cluster. The datasource uses the Snowflake Python API's optimized `read_batch` and `write_pandas` methods to enable parallel read and writes of data.\n",
    "\n",
    "The connector is also a Python context manager, and utilize `with` semantics to define when a connection should be established, db operations commited to the database, and the connection closed. \n",
    "\n",
    "The code below will read from a sample table using the connector to manage the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import SnowflakeConnector\n",
    "\n",
    "# query the number of rows, using the connection context to\n",
    "# manage transactions\n",
    "with SnowflakeConnector(**connect_props) as con:\n",
    "    count = con.query_int(f'SELECT COUNT(*) FROM CUSTOMER')\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use `try` blocks with the connector's `open`, `commit` and `close` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = SnowflakeConnector(**connect_props)\n",
    "try:\n",
    "    connector.open()\n",
    "    count = connector.query_int(f'SELECT COUNT(*) FROM CUSTOMER')\n",
    "finally:\n",
    "    connector.close()\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake Datasource\n",
    "The Snowflake datasource can be used with the Ray data `read_datasource` and `write_datasource` methods to read and write to Snowflake databases using the distibuted processing capabilities of Ray data. The datasource uses a SnowflakeConnector class that is derived from the DBAPI2Connector class. \n",
    "\n",
    "Below is an exmaple of creating the datasource using the previously defined connect properties, and then using it to read and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import read_datasource\n",
    "from ray.data.datasource import SnowflakeDatasource\n",
    "\n",
    "# create a datasource from a connector\n",
    "datasource = SnowflakeDatasource(connector)\n",
    "\n",
    "# use read_datasource to read\n",
    "ds = read_datasource(\n",
    "    datasource, \n",
    "    table='CUSTOMER'\n",
    ")\n",
    " \n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use write_datasource to write\n",
    "connector = SnowflakeConnector(**write_connect_props)\n",
    "datasource = SnowflakeDatasource(connector)\n",
    "ds.write_datasource(\n",
    "    datasource, \n",
    "    table='CUSTOMER_3',\n",
    "    auto_create_table=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DML and DDL\n",
    "The connector can also be used for any DDL or DML operations you would normally execute through the Snowflake Python API. These operations just pass through to the underlying Snowflake API. \n",
    "\n",
    "The code below will create the objects needed for writing to tables. Note that a commit is issued between the queries so the DDL operation executes prior to the next one that is dependent. An alternative is to use two `with` blocks to define transaction boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connector as con:\n",
    "    con.query(f'CREATE DATABASE IF NOT EXISTS RAY')\n",
    "    con.commit()\n",
    "    con.query(f'''\n",
    "        CREATE OR REPLACE TABLE RAY.PUBLIC.CUSTOMER_COPY\n",
    "            LIKE SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n",
    "    ''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas data mapping\n",
    "The Snowflake Datasource converts Pandas data types using the Snowflake Python Connector API. Data mappings are available from the Snowflake [documentation](https://docs.snowflake.com/en/user-guide/python-connector-pandas.html#snowflake-to-pandas-data-mapping). \n",
    "\n",
    "The below code is an example of reading and writing all the available data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connector as con:\n",
    "    con.query(\"\"\"\n",
    "        CREATE OR REPLACE TABLE SAMPLE_TABLE (\n",
    "            ID INT,\n",
    "            SAMPLE_NUMBER NUMBER(6,2),\n",
    "            SAMPLE_DECIMAL DECIMAL(8,3),\n",
    "            SAMPLE_FLOAT FLOAT,\n",
    "            SAMPLE_VARCHAR VARCHAR,\n",
    "            SAMPLE_BINARY BINARY,\n",
    "            SAMPLE_INT INT,\n",
    "            SAMPLE_DATE DATE,\n",
    "            SAMPLE_TIME TIME,\n",
    "            SAMPLE_TIMESTAMP_TZ TIMESTAMP_TZ,\n",
    "            SAMPLE_TIMESTAMP_NTZ TIMESTAMP_NTZ,\n",
    "            SAMPLE_TIMESTAMP_LTZ TIMESTAMP_LTZ,\n",
    "            SAMPLE_GEOGRAPHY GEOGRAPHY,\n",
    "            SAMPLE_VARIANT VARIANT,\n",
    "            SAMPLE_ARRAY ARRAY,\n",
    "            SAMPLE_OBJECT OBJECT\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.commit()\n",
    "    con.query(\"\"\"\n",
    "        INSERT INTO SAMPLE_TABLE \n",
    "        VALUES (\n",
    "            0,\n",
    "            1111.11,\n",
    "            22222.222,\n",
    "            3.333333333,\n",
    "            '4444444444',\n",
    "            '01ffeeddaa',\n",
    "            6666,\n",
    "            TO_DATE('2007-07-07'),\n",
    "            TO_TIME('08:00:00.000'),\n",
    "            TO_TIMESTAMP_TZ('2009-07-08 08:00:00'),\n",
    "            TO_TIMESTAMP_NTZ('2010-07-08 08:00:00.000'),\n",
    "            TO_TIMESTAMP_LTZ('2011-07-08 08:00:00.000'),\n",
    "            'POINT(-122.35 37.55)',\n",
    "            NULL,\n",
    "            NULL,\n",
    "            NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.query(\"\"\"UPDATE SAMPLE_TABLE SET SAMPLE_VARIANT = to_variant(parse_json('{\"key3\": \"value3\", \"key4\": \"value4\"}'))\"\"\")\n",
    "    con.query(\"UPDATE SAMPLE_TABLE SET SAMPLE_ARRAY = [1,'two',3,4]\")\n",
    "    con.query(\"UPDATE SAMPLE_TABLE SET SAMPLE_OBJECT = {'thirteen':13, 'zero':0}\")\n",
    "\n",
    "sample = read_snowflake(write_connect_props, table='SAMPLE_TABLE')\n",
    "sample.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code writes the sample data back to Snowflake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = sample.drop_columns(['SAMPLE_BINARY']) # binary column write does not work in Snowflake API\n",
    "new_sample.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='SAMPLE_TABLE_DEST', \n",
    "    auto_create_table=True\n",
    ")\n",
    "read_snowflake(\n",
    "    write_connect_props, \n",
    "    table='SAMPLE_TABLE_DEST'\n",
    ").to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
