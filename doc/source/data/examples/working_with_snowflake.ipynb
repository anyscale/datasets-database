{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snowflake connector](images/ray_snowflake.png)\n",
    "# Working with Snowflake\n",
    "This user guide walks through the basics of reading and writing data with the Ray Snowflake connector, and using the data for training and tuning an ML model.\n",
    "\n",
    "## Connection properties\n",
    "The Snowflake connection properties need to be provided to the data source upon creation. The minimal required properties are `user`, `password`, `account` and `warehouse`. To use API keys instead of password, functionality to load Snowflake API keys is also provided. API keys can be loaded from a file specified by the `private_key_file` property, or can be passed directly via the `private_key` property. If the key is password protected, the password can be given via the `pk_password` property.  Optional properties like database and schema can also be provided at construction or be included in the fully specified table name of format `db.schema.table` when calling read or write operations with a table or subquery.\n",
    "\n",
    "Below is an example of loading properties from the environment, and filtering them by the 'SNOWFLAKE_' prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment connection properties:\n",
      "account\n",
      "private_key_file\n",
      "pk_password\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "env_connect_props = {\n",
    "    key.replace('SNOWFLAKE_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'SNOWFLAKE_' in key\n",
    "}\n",
    "print('Environment connection properties:')\n",
    "print('\\n'.join(env_connect_props.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from Snowflake\n",
    "Ray will use Snowflake optimizations that allow query results to be read in parallel into a Ray cluster. The created Ray datasets is composed of Pandas dataframes that are spread across the Ray cluster to allow for the distributed operations required in machine learning.\n",
    "\n",
    "![Snowflake read table](images/snowflake_read_table.png)\n",
    "\n",
    "\n",
    "### Read from tables\n",
    "In order to read an entire table into a a Ray cluster, utilize the Ray data `read_snowflake` method. The code below will read in a sample table from the Snowflake sample database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 01:14:54,632\tINFO worker.py:1242 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "find: ‘.git’: No such file or directory\n",
      "2023-02-06 01:14:55,011\tINFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.0.63.233:9031...\n",
      "2023-02-06 01:14:55,228\tINFO worker.py:1544 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_vnmb5jgl4z6q98h61dx25rccju/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-02-06 01:14:55,233\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_0768b49d820a3e94056f2f221c3eba12.zip' (0.38MiB) to Ray cluster...\n",
      "2023-02-06 01:14:55,239\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_0768b49d820a3e94056f2f221c3eba12.zip'.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00,  7.38it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 686.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_CUSTKEY</th>\n",
       "      <th>C_NAME</th>\n",
       "      <th>C_ADDRESS</th>\n",
       "      <th>C_NATIONKEY</th>\n",
       "      <th>C_PHONE</th>\n",
       "      <th>C_ACCTBAL</th>\n",
       "      <th>C_MKTSEGMENT</th>\n",
       "      <th>C_COMMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30001</td>\n",
       "      <td>Customer#000030001</td>\n",
       "      <td>Ui1b,3Q71CiLTJn4MbVp,,YCZARIaNTelfst</td>\n",
       "      <td>4</td>\n",
       "      <td>14-526-204-4500</td>\n",
       "      <td>8848.47</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>frays wake blithely enticingly ironic asymptote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30002</td>\n",
       "      <td>Customer#000030002</td>\n",
       "      <td>UVBoMtILkQu1J3v</td>\n",
       "      <td>11</td>\n",
       "      <td>21-340-653-9800</td>\n",
       "      <td>5221.81</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>he slyly ironic pinto beans wake slyly above t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30003</td>\n",
       "      <td>Customer#000030003</td>\n",
       "      <td>CuGi9fwKn8JdR</td>\n",
       "      <td>21</td>\n",
       "      <td>31-757-493-7525</td>\n",
       "      <td>3014.89</td>\n",
       "      <td>BUILDING</td>\n",
       "      <td>e furiously alongside of the requests. evenly ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_CUSTKEY              C_NAME                             C_ADDRESS  \\\n",
       "0      30001  Customer#000030001  Ui1b,3Q71CiLTJn4MbVp,,YCZARIaNTelfst   \n",
       "1      30002  Customer#000030002                       UVBoMtILkQu1J3v   \n",
       "2      30003  Customer#000030003                         CuGi9fwKn8JdR   \n",
       "\n",
       "   C_NATIONKEY          C_PHONE  C_ACCTBAL C_MKTSEGMENT  \\\n",
       "0            4  14-526-204-4500    8848.47    MACHINERY   \n",
       "1           11  21-340-653-9800    5221.81    MACHINERY   \n",
       "2           21  31-757-493-7525    3014.89     BUILDING   \n",
       "\n",
       "                                           C_COMMENT  \n",
       "0    frays wake blithely enticingly ironic asymptote  \n",
       "1  he slyly ironic pinto beans wake slyly above t...  \n",
       "2  e furiously alongside of the requests. evenly ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cryptography.hazmat.backends import default_backend\n",
    "from ray.data.datasource import DBAPI2Connector\n",
    "from ray.data import read_snowflake\n",
    "# add db and schema in connect props\n",
    "connect_props = dict(\n",
    "    database = 'SNOWFLAKE_SAMPLE_DATA',\n",
    "    schema = 'TPCH_SF1',\n",
    "    warehouse='COMPUTE_WH',\n",
    "    password='C0lumbia!',\n",
    "    **env_connect_props\n",
    ")\n",
    "\n",
    "# read the entire table\n",
    "ds = read_snowflake(connect_props, table='CUSTOMER') \n",
    "\n",
    "# display the first 3 results\n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read with a query\n",
    "For more control over columns and rows read, as well as joining data from multiple tables, a query can be specified instead of a table name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 01:15:01,825\tWARNING read_api.py:332 -- ⚠️  The number of blocks in this dataset (2) limits its parallelism to 2 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00,  7.18it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 922.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_ACCTBAL</th>\n",
       "      <th>C_MKTSEGMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-272.60</td>\n",
       "      <td>BUILDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-78.56</td>\n",
       "      <td>AUTOMOBILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-917.75</td>\n",
       "      <td>FURNITURE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_ACCTBAL C_MKTSEGMENT\n",
       "0    -272.60     BUILDING\n",
       "1     -78.56   AUTOMOBILE\n",
       "2    -917.75    FURNITURE"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = 'SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL < 0'\n",
    "\n",
    "# read the result of the query\n",
    "ds2 = read_snowflake(connect_props, query=QUERY)\n",
    "\n",
    "# display the first 3 results\n",
    "ds2.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional read parameters\n",
    "For reading from Snowflake, underlying Python API arguments are also available. The `timeout` and `params` arguments may be used in the [cursor execute method](https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor).\n",
    "\n",
    "The code below uses the params to specify params to be used by Snowflake when executing the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 660.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_ACCTBAL</th>\n",
       "      <th>C_MKTSEGMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8848.47</td>\n",
       "      <td>MACHINERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5221.81</td>\n",
       "      <td>MACHINERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3014.89</td>\n",
       "      <td>BUILDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_ACCTBAL C_MKTSEGMENT\n",
       "0    8848.47    MACHINERY\n",
       "1    5221.81    MACHINERY\n",
       "2    3014.89     BUILDING"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = 'SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL > ?'\n",
    "\n",
    "ds3 = read_snowflake(connect_props, query=QUERY, params=[1000], timeout=1000)\n",
    "ds3.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing\n",
    "The Ray Snowflake connector will use Snowflake API to write each partition of data in parallel. Each partition of data in the Ray dataset will have a write task that writes in parallel to Snowflake. \n",
    "![Snowflake write table](images/snowflake_write_table.png)\n",
    "\n",
    "### Write to tables\n",
    "In order to write a dataset into Snowflake table, use the `write_snowflake` method of the dataset object. Repartition the dataset in order to set the number of write tasks.\n",
    "\n",
    "First, a new database and table needs to be created using the Snowflake connector API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake import connector\n",
    "\n",
    "write_connect_props = {\n",
    "    **connect_props, \n",
    "    'database':'RAY_SAMPLE', \n",
    "    'schema':'PUBLIC'\n",
    "}\n",
    "with connector.connect(**write_connect_props) as con:\n",
    "    # create destination database\n",
    "    con.cursor().execute(f'CREATE DATABASE IF NOT EXISTS RAY_SAMPLE')\n",
    "    con.commit()\n",
    "    \n",
    "    # create destination table\n",
    "    con.cursor().execute('''\n",
    "        CREATE OR REPLACE TABLE CUSTOMER_COPY \n",
    "        LIKE SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n",
    "    ''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below writes the previously read data into a new database table that are created using the Snowflake Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 19/19 [00:04<00:00,  4.24it/s]\n",
      "Write Progress: 100%|██████████| 19/19 [00:09<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# write the dataset to the table \n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='CUSTOMER_COPY'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional write parameters\n",
    "For writing to Snowflake, the native Snowflake API arguments are also available from the [write_pandas](https://docs.snowflake.com/en/user-guide/python-connector-api.html#module-snowflake-connector-pandas-tools) method. The following is a list of the parameters that may be useful:\n",
    "\n",
    "- `auto_create_table`: When true, will automatically create a table with corresponding columns for each column in the passed in DataFrame. The table will not be created if it already exists\n",
    "- `overwrite`: When true, and if auto_create_table is true, then it drops the table. Otherwise, it truncates the table. In both cases it will replace the existing contents of the table with that of the passed in Pandas DataFrame.\n",
    "- `table_type`: The table type of to-be-created table. The supported table types include ``temp``/``temporary`` and ``transient``. Empty means permanent table as per SQL convention.\n",
    "\n",
    "In the example below, we use the `auto_create_table` parameter to create the output table before writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 19/19 [00:00<00:00, 3205.75it/s]\n",
      "Write Progress: 100%|██████████| 19/19 [00:11<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# write the dataset to the table, using an autocreated table\n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='CUSTOMER_COPY_2',\n",
    "    auto_create_table=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "If more low level access to the Ray Snowflake connector is needed, the underlying `SnowflakConnector` and `SnowflakeDatasource` can be used.\n",
    "\n",
    "### Snowflake Connector\n",
    "The `SnowflakeConnector` class holds the connection properties and logic required to establish a connection with Snowflake. Internally it calls the native Python Snowflake API in order to read and write from and to Snowflake tables in parallel across the cluster. The datasource uses the Snowflake Python API's optimized `read_batch` and `write_pandas` methods to enable parallel read and writes of data.\n",
    "\n",
    "The connector is also a Python context manager, and utilize `with` semantics to define when a connection should be established, db operations commited to the database, and the connection closed. \n",
    "\n",
    "The code below will read from a sample table using the connector to manage the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    }
   ],
   "source": [
    "from ray.data.datasource import SnowflakeConnector\n",
    "\n",
    "# query the number of rows, using the connection context to\n",
    "# manage transactions\n",
    "with SnowflakeConnector(**connect_props) as con:\n",
    "    count = con.query_int(f'SELECT COUNT(*) FROM CUSTOMER')\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use `try` blocks with the connector's `open`, `commit` and `close` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    }
   ],
   "source": [
    "connector = SnowflakeConnector(**connect_props)\n",
    "try:\n",
    "    connector.open()\n",
    "    count = connector.query_int(f'SELECT COUNT(*) FROM CUSTOMER')\n",
    "finally:\n",
    "    connector.close()\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake Datasource\n",
    "The Snowflake datasource can be used with the Ray data `read_datasource` and `write_datasource` methods to read and write to Snowflake databases using the distibuted processing capabilities of Ray data. The datasource uses a SnowflakeConnector class that is derived from the DBAPI2Connector class. \n",
    "\n",
    "Below is an exmaple of creating the datasource using the previously defined connect properties, and then using it to read and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 10.53it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 841.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_CUSTKEY</th>\n",
       "      <th>C_NAME</th>\n",
       "      <th>C_ADDRESS</th>\n",
       "      <th>C_NATIONKEY</th>\n",
       "      <th>C_PHONE</th>\n",
       "      <th>C_ACCTBAL</th>\n",
       "      <th>C_MKTSEGMENT</th>\n",
       "      <th>C_COMMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30001</td>\n",
       "      <td>Customer#000030001</td>\n",
       "      <td>Ui1b,3Q71CiLTJn4MbVp,,YCZARIaNTelfst</td>\n",
       "      <td>4</td>\n",
       "      <td>14-526-204-4500</td>\n",
       "      <td>8848.47</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>frays wake blithely enticingly ironic asymptote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30002</td>\n",
       "      <td>Customer#000030002</td>\n",
       "      <td>UVBoMtILkQu1J3v</td>\n",
       "      <td>11</td>\n",
       "      <td>21-340-653-9800</td>\n",
       "      <td>5221.81</td>\n",
       "      <td>MACHINERY</td>\n",
       "      <td>he slyly ironic pinto beans wake slyly above t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30003</td>\n",
       "      <td>Customer#000030003</td>\n",
       "      <td>CuGi9fwKn8JdR</td>\n",
       "      <td>21</td>\n",
       "      <td>31-757-493-7525</td>\n",
       "      <td>3014.89</td>\n",
       "      <td>BUILDING</td>\n",
       "      <td>e furiously alongside of the requests. evenly ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_CUSTKEY              C_NAME                             C_ADDRESS  \\\n",
       "0      30001  Customer#000030001  Ui1b,3Q71CiLTJn4MbVp,,YCZARIaNTelfst   \n",
       "1      30002  Customer#000030002                       UVBoMtILkQu1J3v   \n",
       "2      30003  Customer#000030003                         CuGi9fwKn8JdR   \n",
       "\n",
       "   C_NATIONKEY          C_PHONE  C_ACCTBAL C_MKTSEGMENT  \\\n",
       "0            4  14-526-204-4500    8848.47    MACHINERY   \n",
       "1           11  21-340-653-9800    5221.81    MACHINERY   \n",
       "2           21  31-757-493-7525    3014.89     BUILDING   \n",
       "\n",
       "                                           C_COMMENT  \n",
       "0    frays wake blithely enticingly ironic asymptote  \n",
       "1  he slyly ironic pinto beans wake slyly above t...  \n",
       "2  e furiously alongside of the requests. evenly ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data.datasource import SnowflakeDatasource\n",
    "from ray.data import read_datasource\n",
    "\n",
    "# create a datasource from a connector\n",
    "datasource = SnowflakeDatasource(connector)\n",
    "\n",
    "# use read_datasource to read\n",
    "ds = read_datasource(\n",
    "    datasource, \n",
    "    table='CUSTOMER'\n",
    ")\n",
    " \n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 19/19 [00:00<00:00, 3238.58it/s]\n",
      "Write Progress: 100%|██████████| 19/19 [00:10<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# use write_datasource to write\n",
    "connector = SnowflakeConnector(**write_connect_props)\n",
    "datasource = SnowflakeDatasource(connector)\n",
    "ds.write_datasource(\n",
    "    datasource, \n",
    "    table='CUSTOMER_3',\n",
    "    auto_create_table=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DML and DDL\n",
    "The connector can also be used for any DDL or DML operations you would normally execute through the Snowflake Python API. These operations just pass through to the underlying Snowflake API. \n",
    "\n",
    "The code below will create the objects needed for writing to tables. Note that a commit is issued between the queries so the DDL operation executes prior to the next one that is dependent. An alternative is to use two `with` blocks to define transaction boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connector as con:\n",
    "    con.query(f'CREATE DATABASE IF NOT EXISTS RAY')\n",
    "    con.commit()\n",
    "    con.query(f'''\n",
    "        CREATE OR REPLACE TABLE RAY.PUBLIC.CUSTOMER_COPY\n",
    "            LIKE SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n",
    "    ''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas data mapping\n",
    "The Snowflake Datasource converts Pandas data types using the Snowflake Python Connector API. Data mappings are available from the Snowflake [documentation](https://docs.snowflake.com/en/user-guide/python-connector-pandas.html#snowflake-to-pandas-data-mapping). \n",
    "\n",
    "The below code is an example of reading and writing all the available data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 01:18:09,888\tWARNING read_api.py:332 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 241.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SAMPLE_NUMBER</th>\n",
       "      <th>SAMPLE_DECIMAL</th>\n",
       "      <th>SAMPLE_FLOAT</th>\n",
       "      <th>SAMPLE_VARCHAR</th>\n",
       "      <th>SAMPLE_BINARY</th>\n",
       "      <th>SAMPLE_INT</th>\n",
       "      <th>SAMPLE_DATE</th>\n",
       "      <th>SAMPLE_TIME</th>\n",
       "      <th>SAMPLE_TIMESTAMP_TZ</th>\n",
       "      <th>SAMPLE_TIMESTAMP_NTZ</th>\n",
       "      <th>SAMPLE_TIMESTAMP_LTZ</th>\n",
       "      <th>SAMPLE_GEOGRAPHY</th>\n",
       "      <th>SAMPLE_VARIANT</th>\n",
       "      <th>SAMPLE_ARRAY</th>\n",
       "      <th>SAMPLE_OBJECT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1111.11</td>\n",
       "      <td>22222.222</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4444444444</td>\n",
       "      <td>b'\\x01\\xff\\xee\\xdd\\xaa'</td>\n",
       "      <td>6666</td>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>2009-07-08 08:00:00-07:00</td>\n",
       "      <td>2010-07-08 08:00:00</td>\n",
       "      <td>2011-07-08 08:00:00-07:00</td>\n",
       "      <td>{\\n  \"coordinates\": [\\n    -122.35,\\n    37.55...</td>\n",
       "      <td>{\\n  \"key3\": \"value3\",\\n  \"key4\": \"value4\"\\n}</td>\n",
       "      <td>[\\n  1,\\n  \"two\",\\n  3,\\n  4\\n]</td>\n",
       "      <td>{\\n  \"thirteen\": 13,\\n  \"zero\": 0\\n}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  SAMPLE_NUMBER  SAMPLE_DECIMAL  SAMPLE_FLOAT SAMPLE_VARCHAR  \\\n",
       "0   0        1111.11       22222.222      3.333333     4444444444   \n",
       "\n",
       "             SAMPLE_BINARY  SAMPLE_INT SAMPLE_DATE SAMPLE_TIME  \\\n",
       "0  b'\\x01\\xff\\xee\\xdd\\xaa'        6666  2007-07-07    08:00:00   \n",
       "\n",
       "        SAMPLE_TIMESTAMP_TZ SAMPLE_TIMESTAMP_NTZ      SAMPLE_TIMESTAMP_LTZ  \\\n",
       "0 2009-07-08 08:00:00-07:00  2010-07-08 08:00:00 2011-07-08 08:00:00-07:00   \n",
       "\n",
       "                                    SAMPLE_GEOGRAPHY  \\\n",
       "0  {\\n  \"coordinates\": [\\n    -122.35,\\n    37.55...   \n",
       "\n",
       "                                  SAMPLE_VARIANT  \\\n",
       "0  {\\n  \"key3\": \"value3\",\\n  \"key4\": \"value4\"\\n}   \n",
       "\n",
       "                      SAMPLE_ARRAY                         SAMPLE_OBJECT  \n",
       "0  [\\n  1,\\n  \"two\",\\n  3,\\n  4\\n]  {\\n  \"thirteen\": 13,\\n  \"zero\": 0\\n}  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with connector as con:\n",
    "    con.query(\"\"\"\n",
    "        CREATE OR REPLACE TABLE SAMPLE_TABLE (\n",
    "            ID INT,\n",
    "            SAMPLE_NUMBER NUMBER(6,2),\n",
    "            SAMPLE_DECIMAL DECIMAL(8,3),\n",
    "            SAMPLE_FLOAT FLOAT,\n",
    "            SAMPLE_VARCHAR VARCHAR,\n",
    "            SAMPLE_BINARY BINARY,\n",
    "            SAMPLE_INT INT,\n",
    "            SAMPLE_DATE DATE,\n",
    "            SAMPLE_TIME TIME,\n",
    "            SAMPLE_TIMESTAMP_TZ TIMESTAMP_TZ,\n",
    "            SAMPLE_TIMESTAMP_NTZ TIMESTAMP_NTZ,\n",
    "            SAMPLE_TIMESTAMP_LTZ TIMESTAMP_LTZ,\n",
    "            SAMPLE_GEOGRAPHY GEOGRAPHY,\n",
    "            SAMPLE_VARIANT VARIANT,\n",
    "            SAMPLE_ARRAY ARRAY,\n",
    "            SAMPLE_OBJECT OBJECT\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.commit()\n",
    "    con.query(\"\"\"\n",
    "        INSERT INTO SAMPLE_TABLE \n",
    "        VALUES (\n",
    "            0,\n",
    "            1111.11,\n",
    "            22222.222,\n",
    "            3.333333333,\n",
    "            '4444444444',\n",
    "            '01ffeeddaa',\n",
    "            6666,\n",
    "            TO_DATE('2007-07-07'),\n",
    "            TO_TIME('08:00:00.000'),\n",
    "            TO_TIMESTAMP_TZ('2009-07-08 08:00:00'),\n",
    "            TO_TIMESTAMP_NTZ('2010-07-08 08:00:00.000'),\n",
    "            TO_TIMESTAMP_LTZ('2011-07-08 08:00:00.000'),\n",
    "            'POINT(-122.35 37.55)',\n",
    "            NULL,\n",
    "            NULL,\n",
    "            NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.query(\"\"\"UPDATE SAMPLE_TABLE SET SAMPLE_VARIANT = to_variant(parse_json('{\"key3\": \"value3\", \"key4\": \"value4\"}'))\"\"\")\n",
    "    con.query(\"UPDATE SAMPLE_TABLE SET SAMPLE_ARRAY = [1,'two',3,4]\")\n",
    "    con.query(\"UPDATE SAMPLE_TABLE SET SAMPLE_OBJECT = {'thirteen':13, 'zero':0}\")\n",
    "\n",
    "sample = read_snowflake(write_connect_props, table='SAMPLE_TABLE')\n",
    "sample.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code writes the sample data back to Snowflake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The size in bytes of the block must be known: (ObjectRef(00ffffffffffffffffffffffffffffffffffffff0300000002000000), BlockMetadata(num_rows=1, size_bytes=None, schema=PandasBlockSchema(names=['ID', 'SAMPLE_NUMBER', 'SAMPLE_DECIMAL', 'SAMPLE_FLOAT', 'SAMPLE_VARCHAR', 'SAMPLE_BINARY', 'SAMPLE_INT', 'SAMPLE_DATE', 'SAMPLE_TIME', 'SAMPLE_TIMESTAMP_TZ', 'SAMPLE_TIMESTAMP_NTZ', 'SAMPLE_TIMESTAMP_LTZ', 'SAMPLE_GEOGRAPHY', 'SAMPLE_VARIANT', 'SAMPLE_ARRAY', 'SAMPLE_OBJECT'], types=[dtype('int64'), dtype('O'), dtype('O'), dtype('float64'), dtype('O'), dtype('O'), dtype('int64'), dtype('O'), dtype('O'), datetime64[ns, pytz.FixedOffset(-420)], dtype('<M8[ns]'), datetime64[ns, America/Los_Angeles], dtype('O'), dtype('O'), dtype('O'), dtype('O')]), input_files=[], exec_stats=None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m new_sample \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39mdrop_columns([\u001b[39m'\u001b[39m\u001b[39mSAMPLE_BINARY\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m# binary column write does not work in Snowflake API\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m new_sample\u001b[39m.\u001b[39;49mwrite_snowflake(\n\u001b[1;32m      3\u001b[0m     write_connect_props, \n\u001b[1;32m      4\u001b[0m     table\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSAMPLE_TABLE_DEST\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m     auto_create_table\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m read_snowflake(\n\u001b[1;32m      8\u001b[0m     write_connect_props, \n\u001b[1;32m      9\u001b[0m     table\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSAMPLE_TABLE_DEST\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\u001b[39m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/dataset.py:2612\u001b[0m, in \u001b[0;36mDataset.write_snowflake\u001b[0;34m(self, connect_properties, table, ray_remote_args, **databrick_args)\u001b[0m\n\u001b[1;32m   2610\u001b[0m connector \u001b[39m=\u001b[39m SnowflakeConnector(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconnect_properties)\n\u001b[1;32m   2611\u001b[0m datasource \u001b[39m=\u001b[39m SnowflakeDatasource(connector)\n\u001b[0;32m-> 2612\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_datasource(\n\u001b[1;32m   2613\u001b[0m     datasource,\n\u001b[1;32m   2614\u001b[0m     table\u001b[39m=\u001b[39;49mtable,\n\u001b[1;32m   2615\u001b[0m     ray_remote_args\u001b[39m=\u001b[39;49mray_remote_args,\n\u001b[1;32m   2616\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdatabrick_args\n\u001b[1;32m   2617\u001b[0m )\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/dataset.py:2717\u001b[0m, in \u001b[0;36mDataset.write_datasource\u001b[0;34m(self, datasource, ray_remote_args, **write_args)\u001b[0m\n\u001b[1;32m   2709\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2710\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe local scheme paths \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m are not supported in Ray Client.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2711\u001b[0m         )\n\u001b[1;32m   2712\u001b[0m     ray_remote_args[\u001b[39m\"\u001b[39m\u001b[39mscheduling_strategy\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m NodeAffinitySchedulingStrategy(\n\u001b[1;32m   2713\u001b[0m         ray\u001b[39m.\u001b[39mget_runtime_context()\u001b[39m.\u001b[39mget_node_id(),\n\u001b[1;32m   2714\u001b[0m         soft\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   2715\u001b[0m     )\n\u001b[0;32m-> 2717\u001b[0m blocks, metadata \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plan\u001b[39m.\u001b[39;49mexecute()\u001b[39m.\u001b[39mget_blocks_with_metadata())\n\u001b[1;32m   2719\u001b[0m \u001b[39m# TODO(ekl) remove this feature flag.\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mRAY_DATASET_FORCE_LOCAL_METADATA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/plan.py:532\u001b[0m, in \u001b[0;36mExecutionPlan.execute\u001b[0;34m(self, allow_clear_input_blocks, force_read)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_internal\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexecution\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy_compat\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    528\u001b[0m     execute_to_legacy_block_list,\n\u001b[1;32m    529\u001b[0m )\n\u001b[1;32m    531\u001b[0m executor \u001b[39m=\u001b[39m BulkExecutor(copy\u001b[39m.\u001b[39mdeepcopy(context\u001b[39m.\u001b[39mexecution_options))\n\u001b[0;32m--> 532\u001b[0m blocks \u001b[39m=\u001b[39m execute_to_legacy_block_list(\n\u001b[1;32m    533\u001b[0m     executor,\n\u001b[1;32m    534\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    535\u001b[0m     allow_clear_input_blocks\u001b[39m=\u001b[39;49mallow_clear_input_blocks,\n\u001b[1;32m    536\u001b[0m     dataset_uuid\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_uuid,\n\u001b[1;32m    537\u001b[0m )\n\u001b[1;32m    538\u001b[0m \u001b[39m# TODO(ekl) we shouldn't need to set this in the future once we move\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39m# to a fully lazy execution model, unless .cache() is used. The reason\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39m# we need it right now is since the user may iterate over a Dataset\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39m# multiple times after fully executing it once.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_by_consumer:\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:83\u001b[0m, in \u001b[0;36mexecute_to_legacy_block_list\u001b[0;34m(executor, plan, allow_clear_input_blocks, dataset_uuid)\u001b[0m\n\u001b[1;32m     81\u001b[0m     dag, stats \u001b[39m=\u001b[39m get_execution_plan(plan\u001b[39m.\u001b[39m_logical_plan)\u001b[39m.\u001b[39mdag, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     dag, stats \u001b[39m=\u001b[39m _to_operator_dag(plan, allow_clear_input_blocks)\n\u001b[1;32m     84\u001b[0m bundles \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mexecute(dag, initial_stats\u001b[39m=\u001b[39mstats)\n\u001b[1;32m     85\u001b[0m _set_stats_uuid_recursive(executor\u001b[39m.\u001b[39mget_stats(), dataset_uuid)\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:104\u001b[0m, in \u001b[0;36m_to_operator_dag\u001b[0;34m(plan, allow_clear_input_blocks)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     owns_blocks \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m operator \u001b[39m=\u001b[39m _blocks_to_input_buffer(blocks, owns_blocks)\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m stage \u001b[39min\u001b[39;00m stages:\n\u001b[1;32m    106\u001b[0m     operator \u001b[39m=\u001b[39m _stage_to_operator(stage, operator)\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:157\u001b[0m, in \u001b[0;36m_blocks_to_input_buffer\u001b[0;34m(blocks, owns_blocks)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m MapOperator\u001b[39m.\u001b[39mcreate(do_read, inputs, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDoRead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m _block_list_to_bundles(blocks, owns_blocks\u001b[39m=\u001b[39;49mowns_blocks)\n\u001b[1;32m    158\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m output:\n\u001b[1;32m    159\u001b[0m         \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m i\u001b[39m.\u001b[39mblocks:\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:276\u001b[0m, in \u001b[0;36m_block_list_to_bundles\u001b[0;34m(blocks, owns_blocks)\u001b[0m\n\u001b[1;32m    273\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m    274\u001b[0m \u001b[39mfor\u001b[39;00m block, meta \u001b[39min\u001b[39;00m blocks\u001b[39m.\u001b[39miter_blocks_with_metadata():\n\u001b[1;32m    275\u001b[0m     output\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 276\u001b[0m         RefBundle(\n\u001b[1;32m    277\u001b[0m             [\n\u001b[1;32m    278\u001b[0m                 (\n\u001b[1;32m    279\u001b[0m                     block,\n\u001b[1;32m    280\u001b[0m                     meta,\n\u001b[1;32m    281\u001b[0m                 )\n\u001b[1;32m    282\u001b[0m             ],\n\u001b[1;32m    283\u001b[0m             owns_blocks\u001b[39m=\u001b[39;49mowns_blocks,\n\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m<string>:5\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, blocks, owns_blocks)\u001b[0m\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces.py:44\u001b[0m, in \u001b[0;36mRefBundle.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(b[\u001b[39m1\u001b[39m], BlockMetadata), b\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m b[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msize_bytes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe size in bytes of the block must be known: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(b)\n\u001b[1;32m     46\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The size in bytes of the block must be known: (ObjectRef(00ffffffffffffffffffffffffffffffffffffff0300000002000000), BlockMetadata(num_rows=1, size_bytes=None, schema=PandasBlockSchema(names=['ID', 'SAMPLE_NUMBER', 'SAMPLE_DECIMAL', 'SAMPLE_FLOAT', 'SAMPLE_VARCHAR', 'SAMPLE_BINARY', 'SAMPLE_INT', 'SAMPLE_DATE', 'SAMPLE_TIME', 'SAMPLE_TIMESTAMP_TZ', 'SAMPLE_TIMESTAMP_NTZ', 'SAMPLE_TIMESTAMP_LTZ', 'SAMPLE_GEOGRAPHY', 'SAMPLE_VARIANT', 'SAMPLE_ARRAY', 'SAMPLE_OBJECT'], types=[dtype('int64'), dtype('O'), dtype('O'), dtype('float64'), dtype('O'), dtype('O'), dtype('int64'), dtype('O'), dtype('O'), datetime64[ns, pytz.FixedOffset(-420)], dtype('<M8[ns]'), datetime64[ns, America/Los_Angeles], dtype('O'), dtype('O'), dtype('O'), dtype('O')]), input_files=[], exec_stats=None))"
     ]
    }
   ],
   "source": [
    "new_sample = sample.drop_columns(['SAMPLE_BINARY']) # binary column write does not work in Snowflake API\n",
    "new_sample.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='SAMPLE_TABLE_DEST', \n",
    "    auto_create_table=True\n",
    ")\n",
    "read_snowflake(\n",
    "    write_connect_props, \n",
    "    table='SAMPLE_TABLE_DEST'\n",
    ").to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
