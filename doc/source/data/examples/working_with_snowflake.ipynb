{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snowflake connector](images/ray_snowflake.png)\n",
    "# Working with Snowflake\n",
    "This user guide walks through the basics of reading and writing data with the Ray Snowflake connector, and using the data for training and tuning an ML model.\n",
    "\n",
    "## Connection properties\n",
    "The Snowflake connection properties need to be provided to the data source upon creation. The minimal required properties are `user`, `password`, `account` and `warehouse`. To use API keys instead of password, functionality to load Snowflake API keys is also provided. API keys can be loaded from a file specified by the `private_key_file` property, or can be passed directly via the `private_key` property. If the key is password protected, the password can be given via the `pk_password` property.  Optional properties like database and schema can also be provided at construction or be included in the fully specified table name of format `db.schema.table` when calling read or write operations with a table or subquery.\n",
    "\n",
    "Below is an example of loading properties from the environment, and filtering them by the 'SNOWFLAKE_' prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment connection properties:\n",
      "account\n",
      "private_key_file\n",
      "pk_password\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "env_connect_props = {\n",
    "    key.replace('SNOWFLAKE_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'SNOWFLAKE_' in key\n",
    "}\n",
    "print('Environment connection properties:')\n",
    "print('\\n'.join(env_connect_props.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from Snowflake\n",
    "Ray will use Snowflake optimizations that allow query results to be read in parallel into a Ray cluster. The created Ray datasets is composed of Pandas dataframes that are spread across the Ray cluster to allow for the distributed operations required in machine learning.\n",
    "\n",
    "![Snowflake read table](images/snowflake_read_table.png)\n",
    "\n",
    "\n",
    "### Read from tables\n",
    "In order to read an entire table into a a Ray cluster, utilize the Ray data `read_snowflake` method. The code below will read in a sample table from the Snowflake sample database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 19:58:56,544\tINFO worker.py:1242 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "2023-02-05 19:58:56,872\tINFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.0.63.233:9031...\n",
      "2023-02-05 19:58:56,879\tINFO worker.py:1544 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_vnmb5jgl4z6q98h61dx25rccju/services?redirect_to=dashboard \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Version mismatch: The cluster was started with:\n    Ray: 2.2.0\n    Python: 3.10.8\nThis process on node 10.0.63.233 was started with:\n    Ray: 3.0.0.dev0\n    Python: 3.10.8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m connect_props \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m      6\u001b[0m     database \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSNOWFLAKE_SAMPLE_DATA\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     schema \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTPCH_SF1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39menv_connect_props\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39m# read the entire table\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m ds \u001b[39m=\u001b[39m read_snowflake(connect_props, table\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mCUSTOMER\u001b[39;49m\u001b[39m'\u001b[39;49m) \n\u001b[1;32m     16\u001b[0m \u001b[39m# display the first 3 results\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ds\u001b[39m.\u001b[39mlimit(\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/data/read_api.py:497\u001b[0m, in \u001b[0;36mread_snowflake\u001b[0;34m(connect_properties, table, query, parallelism, ray_remote_args, **snowflake_args)\u001b[0m\n\u001b[1;32m    495\u001b[0m connector \u001b[39m=\u001b[39m SnowflakeConnector(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconnect_properties)\n\u001b[1;32m    496\u001b[0m datasource \u001b[39m=\u001b[39m SnowflakeDatasource(connector)\n\u001b[0;32m--> 497\u001b[0m \u001b[39mreturn\u001b[39;00m read_datasource(\n\u001b[1;32m    498\u001b[0m     datasource,\n\u001b[1;32m    499\u001b[0m     table\u001b[39m=\u001b[39;49mtable,\n\u001b[1;32m    500\u001b[0m     query\u001b[39m=\u001b[39;49mquery,\n\u001b[1;32m    501\u001b[0m     parallelism \u001b[39m=\u001b[39;49m parallelism,\n\u001b[1;32m    502\u001b[0m     ray_remote_args\u001b[39m=\u001b[39;49mray_remote_args,\n\u001b[1;32m    503\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msnowflake_args,\n\u001b[1;32m    504\u001b[0m )\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/data/read_api.py:286\u001b[0m, in \u001b[0;36mread_datasource\u001b[0;34m(datasource, parallelism, ray_remote_args, **read_args)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39m# TODO(ekl) remove this feature flag.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m force_local \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRAY_DATASET_FORCE_LOCAL_METADATA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron\n\u001b[0;32m--> 286\u001b[0m cur_pg \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mutil\u001b[39m.\u001b[39;49mget_current_placement_group()\n\u001b[1;32m    287\u001b[0m pa_ds \u001b[39m=\u001b[39m _lazy_import_pyarrow_dataset()\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m pa_ds:\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/util/placement_group.py:316\u001b[0m, in \u001b[0;36mget_current_placement_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m@PublicAPI\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_current_placement_group\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[PlacementGroup]:\n\u001b[1;32m    287\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the current placement group which a task or actor is using.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[39m    It returns None if there's no current placement group for the worker.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m            created with any placement group.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[39mif\u001b[39;00m client_mode_should_convert(auto_init\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m):\n\u001b[1;32m    317\u001b[0m         \u001b[39m# Client mode is only a driver.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     worker \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39m_private\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39mglobal_worker\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/_private/client_mode_hook.py:124\u001b[0m, in \u001b[0;36mclient_mode_should_convert\u001b[0;34m(auto_init)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mray\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    121\u001b[0m         os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mRAY_ENABLE_AUTO_CONNECT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ray\u001b[39m.\u001b[39mis_initialized()\n\u001b[1;32m    123\u001b[0m     ):\n\u001b[0;32m--> 124\u001b[0m         ray\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m    126\u001b[0m \u001b[39m# `is_client_mode_enabled_by_default` is used for testing with\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m# `RAY_CLIENT_MODE=1`. This flag means all tests run with client mode.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    129\u001b[0m     is_client_mode_enabled \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default\n\u001b[1;32m    130\u001b[0m ) \u001b[39mand\u001b[39;00m _get_client_hook_status_on_thread()\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/_private/worker.py:1555\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     logger\u001b[39m.\u001b[39minfo(info_str)\n\u001b[0;32m-> 1555\u001b[0m connect(\n\u001b[1;32m   1556\u001b[0m     _global_node,\n\u001b[1;32m   1557\u001b[0m     _global_node\u001b[39m.\u001b[39;49msession_name,\n\u001b[1;32m   1558\u001b[0m     mode\u001b[39m=\u001b[39;49mdriver_mode,\n\u001b[1;32m   1559\u001b[0m     log_to_driver\u001b[39m=\u001b[39;49mlog_to_driver,\n\u001b[1;32m   1560\u001b[0m     worker\u001b[39m=\u001b[39;49mglobal_worker,\n\u001b[1;32m   1561\u001b[0m     driver_object_store_memory\u001b[39m=\u001b[39;49m_driver_object_store_memory,\n\u001b[1;32m   1562\u001b[0m     job_id\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1563\u001b[0m     namespace\u001b[39m=\u001b[39;49mnamespace,\n\u001b[1;32m   1564\u001b[0m     job_config\u001b[39m=\u001b[39;49mjob_config,\n\u001b[1;32m   1565\u001b[0m     entrypoint\u001b[39m=\u001b[39;49mray\u001b[39m.\u001b[39;49m_private\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mget_entrypoint_name(),\n\u001b[1;32m   1566\u001b[0m )\n\u001b[1;32m   1567\u001b[0m \u001b[39mif\u001b[39;00m job_config \u001b[39mand\u001b[39;00m job_config\u001b[39m.\u001b[39mcode_search_path:\n\u001b[1;32m   1568\u001b[0m     global_worker\u001b[39m.\u001b[39mset_load_code_from_local(\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/_private/worker.py:1975\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(node, session_name, mode, log_to_driver, worker, driver_object_store_memory, job_id, namespace, job_config, runtime_env_hash, startup_token, ray_debugger_external, entrypoint)\u001b[0m\n\u001b[1;32m   1973\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1974\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m SCRIPT_MODE:\n\u001b[0;32m-> 1975\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1976\u001b[0m     \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m WORKER_MODE:\n\u001b[1;32m   1977\u001b[0m         traceback_str \u001b[39m=\u001b[39m traceback\u001b[39m.\u001b[39mformat_exc()\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/_private/worker.py:1972\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(node, session_name, mode, log_to_driver, worker, driver_object_store_memory, job_id, namespace, job_config, runtime_env_hash, startup_token, ray_debugger_external, entrypoint)\u001b[0m\n\u001b[1;32m   1969\u001b[0m \u001b[39m# For driver's check that the version information matches the version\u001b[39;00m\n\u001b[1;32m   1970\u001b[0m \u001b[39m# information that the Ray cluster was started with.\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1972\u001b[0m     node\u001b[39m.\u001b[39;49mcheck_version_info()\n\u001b[1;32m   1973\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1974\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m SCRIPT_MODE:\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/_private/node.py:345\u001b[0m, in \u001b[0;36mNode.check_version_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m cluster_metadata:\n\u001b[1;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m ray\u001b[39m.\u001b[39;49m_private\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcheck_version_info(cluster_metadata)\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/_private/utils.py:1564\u001b[0m, in \u001b[0;36mcheck_version_info\u001b[0;34m(cluster_metadata)\u001b[0m\n\u001b[1;32m   1555\u001b[0m node_ip_address \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39m_private\u001b[39m.\u001b[39mservices\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[1;32m   1556\u001b[0m error_message \u001b[39m=\u001b[39m (\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mVersion mismatch: The cluster was started with:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    Ray: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m cluster_version_info[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    Python: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m version_info[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1563\u001b[0m )\n\u001b[0;32m-> 1564\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(error_message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Version mismatch: The cluster was started with:\n    Ray: 2.2.0\n    Python: 3.10.8\nThis process on node 10.0.63.233 was started with:\n    Ray: 3.0.0.dev0\n    Python: 3.10.8\n"
     ]
    }
   ],
   "source": [
    "from cryptography.hazmat.backends import default_backend\n",
    "from ray.data.datasource import DBAPI2Connector\n",
    "from ray.data import read_snowflake\n",
    "# add db and schema in connect props\n",
    "connect_props = dict(\n",
    "    database = 'SNOWFLAKE_SAMPLE_DATA',\n",
    "    schema = 'TPCH_SF1',\n",
    "    warehouse='COMPUTE_WH',\n",
    "    password='C0lumbia!',\n",
    "    **env_connect_props\n",
    ")\n",
    "\n",
    "# read the entire table\n",
    "ds = read_snowflake(connect_props, table='CUSTOMER') \n",
    "\n",
    "# display the first 3 results\n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read with a query\n",
    "For more control over columns and rows read, as well as joining data from multiple tables, a query can be specified instead of a table name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = 'SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL < 0'\n",
    "\n",
    "# read the result of the query\n",
    "ds2 = read_snowflake(connect_props, query=QUERY)\n",
    "\n",
    "# display the first 3 results\n",
    "ds2.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional read parameters\n",
    "For reading from Snowflake, underlying Python API arguments are also available. The `timeout` and `params` arguments may be used in the [cursor execute method](https://docs.snowflake.com/en/user-guide/python-connector-api.html#object-cursor).\n",
    "\n",
    "The code below uses the params to specify params to be used by Snowflake when executing the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Worker' object has no attribute 'core_worker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m QUERY \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL > ?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m ds3 \u001b[39m=\u001b[39m read_snowflake(connect_props, query\u001b[39m=\u001b[39;49mQUERY, params\u001b[39m=\u001b[39;49m[\u001b[39m1000\u001b[39;49m], timeout\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m ds3\u001b[39m.\u001b[39mlimit(\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/data/read_api.py:497\u001b[0m, in \u001b[0;36mread_snowflake\u001b[0;34m(connect_properties, table, query, parallelism, ray_remote_args, **snowflake_args)\u001b[0m\n\u001b[1;32m    495\u001b[0m connector \u001b[39m=\u001b[39m SnowflakeConnector(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconnect_properties)\n\u001b[1;32m    496\u001b[0m datasource \u001b[39m=\u001b[39m SnowflakeDatasource(connector)\n\u001b[0;32m--> 497\u001b[0m \u001b[39mreturn\u001b[39;00m read_datasource(\n\u001b[1;32m    498\u001b[0m     datasource,\n\u001b[1;32m    499\u001b[0m     table\u001b[39m=\u001b[39;49mtable,\n\u001b[1;32m    500\u001b[0m     query\u001b[39m=\u001b[39;49mquery,\n\u001b[1;32m    501\u001b[0m     parallelism \u001b[39m=\u001b[39;49m parallelism,\n\u001b[1;32m    502\u001b[0m     ray_remote_args\u001b[39m=\u001b[39;49mray_remote_args,\n\u001b[1;32m    503\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msnowflake_args,\n\u001b[1;32m    504\u001b[0m )\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/data/read_api.py:286\u001b[0m, in \u001b[0;36mread_datasource\u001b[0;34m(datasource, parallelism, ray_remote_args, **read_args)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39m# TODO(ekl) remove this feature flag.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m force_local \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRAY_DATASET_FORCE_LOCAL_METADATA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron\n\u001b[0;32m--> 286\u001b[0m cur_pg \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mutil\u001b[39m.\u001b[39;49mget_current_placement_group()\n\u001b[1;32m    287\u001b[0m pa_ds \u001b[39m=\u001b[39m _lazy_import_pyarrow_dataset()\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m pa_ds:\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/util/placement_group.py:321\u001b[0m, in \u001b[0;36mget_current_placement_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m worker \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39m_private\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39mglobal_worker\n\u001b[1;32m    320\u001b[0m worker\u001b[39m.\u001b[39mcheck_connected()\n\u001b[0;32m--> 321\u001b[0m pg_id \u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39;49mplacement_group_id\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m pg_id\u001b[39m.\u001b[39mis_nil():\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/efs/users/eric_greene_a9c7590/projects/ray/python/ray/_private/worker.py:498\u001b[0m, in \u001b[0;36mWorker.placement_group_id\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplacement_group_id\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcore_worker\u001b[39m.\u001b[39mget_placement_group_id()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Worker' object has no attribute 'core_worker'"
     ]
    }
   ],
   "source": [
    "QUERY = 'SELECT C_ACCTBAL, C_MKTSEGMENT FROM CUSTOMER WHERE C_ACCTBAL > ?'\n",
    "\n",
    "ds3 = read_snowflake(connect_props, query=QUERY, params=[1000], timeout=1000)\n",
    "ds3.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing\n",
    "The Ray Snowflake connector will use Snowflake API to write each partition of data in parallel. Each partition of data in the Ray dataset will have a write task that writes in parallel to Snowflake. \n",
    "![Snowflake write table](images/snowflake_write_table.png)\n",
    "\n",
    "### Write to tables\n",
    "In order to write a dataset into Snowflake table, use the `write_snowflake` method of the dataset object. Repartition the dataset in order to set the number of write tasks.\n",
    "\n",
    "First, a new database and table needs to be created using the Snowflake connector API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake import connector\n",
    "\n",
    "write_connect_props = {\n",
    "    **connect_props, \n",
    "    'database':'RAY_SAMPLE', \n",
    "    'schema':'PUBLIC'\n",
    "}\n",
    "with connector.connect(**write_connect_props) as con:\n",
    "    # create destination database\n",
    "    con.cursor().execute(f'CREATE DATABASE IF NOT EXISTS RAY_SAMPLE')\n",
    "    con.commit()\n",
    "    \n",
    "    # create destination table\n",
    "    con.cursor().execute('''\n",
    "        CREATE OR REPLACE TABLE CUSTOMER_COPY \n",
    "        LIKE SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n",
    "    ''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below writes the previously read data into a new database table that are created using the Snowflake Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataset to the table \n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='CUSTOMER_COPY'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional write parameters\n",
    "For writing to Snowflake, the native Snowflake API arguments are also available from the [write_pandas](https://docs.snowflake.com/en/user-guide/python-connector-api.html#module-snowflake-connector-pandas-tools) method. The following is a list of the parameters that may be useful:\n",
    "\n",
    "- `auto_create_table`: When true, will automatically create a table with corresponding columns for each column in the passed in DataFrame. The table will not be created if it already exists\n",
    "- `overwrite`: When true, and if auto_create_table is true, then it drops the table. Otherwise, it truncates the table. In both cases it will replace the existing contents of the table with that of the passed in Pandas DataFrame.\n",
    "- `table_type`: The table type of to-be-created table. The supported table types include ``temp``/``temporary`` and ``transient``. Empty means permanent table as per SQL convention.\n",
    "\n",
    "In the example below, we use the `auto_create_table` parameter to create the output table before writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataset to the table, using an autocreated table\n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='CUSTOMER_COPY_2',\n",
    "    auto_create_table=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "If more low level access to the Ray Snowflake connector is needed, the underlying `SnowflakConnector` and `SnowflakeDatasource` can be used.\n",
    "\n",
    "### Snowflake Connector\n",
    "The `SnowflakeConnector` class holds the connection properties and logic required to establish a connection with Snowflake. Internally it calls the native Python Snowflake API in order to read and write from and to Snowflake tables in parallel across the cluster. The datasource uses the Snowflake Python API's optimized `read_batch` and `write_pandas` methods to enable parallel read and writes of data.\n",
    "\n",
    "The connector is also a Python context manager, and utilize `with` semantics to define when a connection should be established, db operations commited to the database, and the connection closed. \n",
    "\n",
    "The code below will read from a sample table using the connector to manage the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import SnowflakeConnector\n",
    "\n",
    "# query the number of rows, using the connection context to\n",
    "# manage transactions\n",
    "with SnowflakeConnector(**connect_props) as con:\n",
    "    count = con.query_int(f'SELECT COUNT(*) FROM CUSTOMER')\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use `try` blocks with the connector's `open`, `commit` and `close` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = SnowflakeConnector(**connect_props)\n",
    "try:\n",
    "    connector.open()\n",
    "    count = connector.query_int(f'SELECT COUNT(*) FROM CUSTOMER')\n",
    "finally:\n",
    "    connector.close()\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake Datasource\n",
    "The Snowflake datasource can be used with the Ray data `read_datasource` and `write_datasource` methods to read and write to Snowflake databases using the distibuted processing capabilities of Ray data. The datasource uses a SnowflakeConnector class that is derived from the DBAPI2Connector class. \n",
    "\n",
    "Below is an exmaple of creating the datasource using the previously defined connect properties, and then using it to read and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import SnowflakeDatasource\n",
    "from ray.data import read_datasource\n",
    "\n",
    "# create a datasource from a connector\n",
    "datasource = SnowflakeDatasource(connector)\n",
    "\n",
    "# use read_datasource to read\n",
    "ds = read_datasource(\n",
    "    datasource, \n",
    "    table='CUSTOMER'\n",
    ")\n",
    " \n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use write_datasource to write\n",
    "ds.write_datasource(\n",
    "    datasource, \n",
    "    table='CUSTOMER_3',\n",
    "    auto_create_table=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DML and DDL\n",
    "The connector can also be used for any DDL or DML operations you would normally execute through the Snowflake Python API. These operations just pass through to the underlying Snowflake API. \n",
    "\n",
    "The code below will create the objects needed for writing to tables. Note that a commit is issued between the queries so the DDL operation executes prior to the next one that is dependent. An alternative is to use two `with` blocks to define transaction boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SnowflakeConnector(**write_connect_props) as con:\n",
    "    con.query(f'CREATE DATABASE IF NOT EXISTS RAY')\n",
    "    con.commit()\n",
    "    con.query(f'''\n",
    "        CREATE OR REPLACE TABLE RAY.PUBLIC.CUSTOMER_COPY\n",
    "            LIKE SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n",
    "    ''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas data mapping\n",
    "The Snowflake Datasource converts Pandas data types using the Snowflake Python Connector API. Data mappings are available from the Snowflake [documentation](https://docs.snowflake.com/en/user-guide/python-connector-pandas.html#snowflake-to-pandas-data-mapping). \n",
    "\n",
    "The below code is an example of reading and writing all the available data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SnowflakeConnector(**write_connect_props) as con:\n",
    "    con.query(\"\"\"\n",
    "        CREATE OR REPLACE TABLE SAMPLE_TABLE (\n",
    "            ID INT,\n",
    "            SAMPLE_NUMBER NUMBER(6,2),\n",
    "            SAMPLE_DECIMAL DECIMAL(8,3),\n",
    "            SAMPLE_FLOAT FLOAT,\n",
    "            SAMPLE_VARCHAR VARCHAR,\n",
    "            SAMPLE_BINARY BINARY,\n",
    "            SAMPLE_INT INT,\n",
    "            SAMPLE_DATE DATE,\n",
    "            SAMPLE_TIME TIME,\n",
    "            SAMPLE_TIMESTAMP_TZ TIMESTAMP_TZ,\n",
    "            SAMPLE_TIMESTAMP_NTZ TIMESTAMP_NTZ,\n",
    "            SAMPLE_TIMESTAMP_LTZ TIMESTAMP_LTZ,\n",
    "            SAMPLE_GEOGRAPHY GEOGRAPHY,\n",
    "            SAMPLE_VARIANT VARIANT,\n",
    "            SAMPLE_ARRAY ARRAY,\n",
    "            SAMPLE_OBJECT OBJECT\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.commit()\n",
    "    con.query(\"\"\"\n",
    "        INSERT INTO SAMPLE_TABLE \n",
    "        VALUES (\n",
    "            0,\n",
    "            1111.11,\n",
    "            22222.222,\n",
    "            3.333333333,\n",
    "            '4444444444',\n",
    "            '01ffeeddaa',\n",
    "            6666,\n",
    "            TO_DATE('2007-07-07'),\n",
    "            TO_TIME('08:00:00.000'),\n",
    "            TO_TIMESTAMP_TZ('2009-07-08 08:00:00'),\n",
    "            TO_TIMESTAMP_NTZ('2010-07-08 08:00:00.000'),\n",
    "            TO_TIMESTAMP_LTZ('2011-07-08 08:00:00.000'),\n",
    "            'POINT(-122.35 37.55)',\n",
    "            NULL,\n",
    "            NULL,\n",
    "            NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.query(\"\"\"UPDATE SAMPLE_TABLE SET SAMPLE_VARIANT = to_variant(parse_json('{\"key3\": \"value3\", \"key4\": \"value4\"}'))\"\"\")\n",
    "    con.query(\"UPDATE SAMPLE_TABLE SET SAMPLE_ARRAY = [1,'two',3,4]\")\n",
    "    con.query(\"UPDATE SAMPLE_TABLE SET SAMPLE_OBJECT = {'thirteen':13, 'zero':0}\")\n",
    "\n",
    "sample = read_snowflake(write_connect_props, table='SAMPLE_TABLE')\n",
    "sample.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code writes the sample data back to Snowflake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = sample.drop_columns(['SAMPLE_BINARY']) # binary column write does not work in Snowflake API\n",
    "new_sample.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='SAMPLE_TABLE_DEST', \n",
    "    auto_create_table=True\n",
    ")\n",
    "read_snowflake(\n",
    "    write_connect_props, \n",
    "    table='SAMPLE_TABLE_DEST'\n",
    ").to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
