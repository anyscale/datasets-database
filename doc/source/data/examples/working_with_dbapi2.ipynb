{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Ray Data DB API 2 Connector](images/dbapi2_connector_logo.png)\n",
    "This user guide walks through the basics of reading and writing data with the Ray DB API 2 connector.\n",
    "\n",
    "The Ray DB API2 connector enables parallel read and write to and from SQL compliant datastores with Ray datasets. The connector utilizes the Python DB API 2.0 specification implemented by most databases. DB API 2 providers include:\n",
    "\n",
    "* SQLite 3 (this example)\n",
    "* Databricks (see [example](working_with_databricks.ipynb))\n",
    "* Snowflake (see [example](working_with_snowflake.ipynb))\n",
    "* Google Big Query\n",
    "* Microsoft SQL Server\n",
    "* Teradata\n",
    "* Postgres\n",
    "* MySQL\n",
    "* Oracle\n",
    "* Many more ...\n",
    "\n",
    "## Connection properties\n",
    "The databsae connection properties need to be provided to the data source upon creation. These properties are documented by the database provider, but for this  example we will use SQLite implementation which is provided in Python.\n",
    "\n",
    "Below is an example of loading properties from the environment, and filtering them by the 'SQLITE_' prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection properties:\n",
      "database\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "env_connect_props = {\n",
    "    key.replace('SQLITE_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'SQLITE_' in key\n",
    "}\n",
    "\n",
    "if not os.path.exists('/mnt/cluster_storage/sqlite'):\n",
    "    os.makedirs('/mnt/cluster_storage/sqlite')\n",
    "\n",
    "# add db and schema in connect props\n",
    "connect_props = {\n",
    "    **env_connect_props,\n",
    "    'database':'/mnt/cluster_storage/sqlite/ray.db'\n",
    "}\n",
    "\n",
    "print('Connection properties:')\n",
    "print('\\n'.join(connect_props.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading\n",
    "Ray will use SQLite Python API to read in parallel into a Ray cluster. The created Ray datasets is composed of PyArrow dataframes that are spread across the Ray cluster to allow for the distributed operations required in machine learning.\n",
    "\n",
    "![DB API 2 read](images/dbapi2_read.png)\n",
    "\n",
    "\n",
    "### Read from tables\n",
    "In order to read an entire table into a a Ray cluster, utilize the Ray data `read_dbapi2` method. The code below will read in a sample table from an SQLite sample database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 15:42:39,644\tINFO worker.py:1242 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "find: ‘.git’: No such file or directory\n",
      "2023-02-21 15:42:39,911\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.17.214:9031...\n",
      "2023-02-21 15:42:39,917\tINFO worker.py:1553 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_vnmb5jgl4z6q98h61dx25rccju/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-02-21 15:42:39,921\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_b6c745db96bcede31d428b5202d682f7.zip' (0.82MiB) to Ray cluster...\n",
      "2023-02-21 15:42:39,932\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_b6c745db96bcede31d428b5202d682f7.zip'.\n",
      "Read progress: 100%|██████████| 10/10 [00:00<00:00, 24.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_val</th>\n",
       "      <th>str_val</th>\n",
       "      <th>flt_val</th>\n",
       "      <th>bool_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_val str_val  flt_val  bool_val\n",
       "0        0       0      0.0         1\n",
       "1        1       1      1.0         0\n",
       "2        2       2      2.0         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlite3 import connect as connect_fn\n",
    "from ray.data import read_dbapi2\n",
    "\n",
    "# create sample data\n",
    "SAMPLE_SIZE = 10\n",
    "with connect_fn(**connect_props) as con:\n",
    "    con.execute('DROP TABLE IF EXISTS source')\n",
    "    con.execute('CREATE TABLE source(int_val, str_val, flt_val, bool_val)')\n",
    "    con.commit()\n",
    "    data = [[int(i),str(i),float(i),i%2==0] for i in range(0,SAMPLE_SIZE)]\n",
    "    con.executemany('INSERT INTO source VALUES (?, ?, ?, ?)', data)\n",
    "\n",
    "# read the entire table\n",
    "ds = read_dbapi2(connect_fn, connect_props, table='source') \n",
    "\n",
    "# get all partitions\n",
    "ds = ds.fully_executed()\n",
    "\n",
    "# display the first 3 results\n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read with a query\n",
    "For more control over columns and rows read, as well as joining data from multiple tables, a query can be specified instead of a table name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 15:42:42,412\tWARNING read_api.py:331 -- ⚠️  The number of blocks in this dataset (3) limits its parallelism to 3 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 3/3 [00:00<00:00, 24.69it/s]\n",
      "Read progress: 100%|██████████| 3/3 [00:00<00:00, 2085.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_val</th>\n",
       "      <th>str_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_val str_val\n",
       "0        3       3\n",
       "1        4       4\n",
       "2        5       5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = 'SELECT int_val, str_val FROM source WHERE int_val >= 3 AND int_val <= 5'\n",
    "\n",
    "# read the result of the query\n",
    "ds2 = read_dbapi2(connect_fn, connect_props, query=QUERY)\n",
    "\n",
    "# display the first 3 results\n",
    "ds2.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional read parameters\n",
    "For reading from Snowflake, underlying Python API arguments are also available. The `args` parameter will be passed to the underlying\n",
    "execute method. Other drivers may require kwargs whihc can be passed as key word arguments to the `read_dbapi2` method.\n",
    "\n",
    "The code below uses the args to specify parameterss to be used by SQLite when executing the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 3/3 [00:00<00:00, 18.20it/s]\n",
      "Read progress: 100%|██████████| 3/3 [00:00<00:00, 1881.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_val</th>\n",
       "      <th>str_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_val str_val\n",
       "0        6       6\n",
       "1        7       7\n",
       "2        8       8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = 'SELECT int_val, str_val FROM source WHERE int_val > ?'\n",
    "\n",
    "ds3 = read_dbapi2(connect_fn, connect_props, query=QUERY, query_args=[[5]])\n",
    "ds3.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing\n",
    "The Ray DB API 2 connector will use the DB API driver to write each partition of data in parallel. Each partition of data in the Ray dataset will have a write task that writes in parallel to Snowflake.\n",
    "![DB API 2 write](images/dbapi2_write.png)\n",
    "\n",
    "### Write to tables\n",
    "In order to write a dataset into database table, use the `write_dbapi2` method of the dataset object. Repartition the dataset prior to calling this method in order to set the number of write tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the destination table\n",
    "with connect_fn(**connect_props) as con:\n",
    "    con.execute('DROP TABLE IF EXISTS destination')\n",
    "    con.execute('CREATE TABLE destination(int_val, str_val, flt_val, bool_val)')\n",
    "    con.execute('DROP TABLE IF EXISTS destination2')\n",
    "    con.execute('CREATE TABLE destination2(int_val, str_val, flt_val, bool_val)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below writes the previously read data into a new database table that are created using the Snowflake Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 15:42:43,447\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[write]\n",
      "write: 100%|██████████| 10/10 [00:01<00:00,  6.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# write the dataset to the table \n",
    "ds.write_dbapi2(connect_fn, connect_props, table='destination')\n",
    "\n",
    "#read_dbapi2(connect_fn, connect_props, table='destination').limit(3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 3/3 [00:00<00:00, 35.55it/s]\n",
      "Read progress: 100%|██████████| 3/3 [00:00<00:00, 1981.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_val</th>\n",
       "      <th>str_val</th>\n",
       "      <th>flt_val</th>\n",
       "      <th>bool_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_val str_val  flt_val  bool_val\n",
       "0        0       0      0.0         1\n",
       "1        1       1      1.0         0\n",
       "2        2       2      2.0         1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_dbapi2(connect_fn, connect_props, table='destination').limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing with stage tables\n",
    "Some databases may lock tables during writing which would significantly increase the overal write time. To get around this, the write mode can be set to stage, wich will cause each partition to be writtent its own stage table. After all data is written to stage tables, they are then copied to the main table. In order for this to work, the database must support the defualt `CREATE TABLE <STAGE> LIKE <DESTINATION>` semantics. \n",
    "\n",
    "![DB API 2 write](images/dbapi2_write_staged.png)\n",
    "\n",
    "> Note: SQLite doesn't support `CREATE TABLE <STAGE> LIKE <DESTINATION>`. Since these semantics are not supported, the below overrides the prepare query to provide the template of sql for creating the stage table. `{table}` will be replaced with the destination table name, and `{block_id}` will be replaced with a unique id for the data block/partition. Many databases may support this, so the `write_queries` parameter would not be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 15:42:45,448\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[write]\n",
      "write: 100%|██████████| 10/10 [00:02<00:00,  3.58it/s]\n",
      "Read progress: 100%|██████████| 3/3 [00:00<00:00,  6.76it/s]\n",
      "Read progress: 100%|██████████| 3/3 [00:00<00:00, 1569.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_val</th>\n",
       "      <th>str_val</th>\n",
       "      <th>flt_val</th>\n",
       "      <th>bool_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_val str_val  flt_val  bool_val\n",
       "0        0       0      0.0         1\n",
       "1        1       1      1.0         0\n",
       "2        2       2      2.0         1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write the dataset to the table\n",
    "ds.write_dbapi2(\n",
    "    connect_fn,\n",
    "    connect_props, \n",
    "    table='destination2',\n",
    "    mode='stage'\n",
    ")\n",
    "\n",
    "read_dbapi2(connect_fn, connect_props, table='destination2').limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "If more low level access to the Ray DB API 2 connector is needed, the underlying `DBAPI2Connector` and `DBAPI2Datasource` can be used.\n",
    "\n",
    "### DB API 2 Connector\n",
    "The `DBAPI2Connector` class holds the connection properties and logic required to establish a connection with a DB API 2 driver for a database. Internally it calls the native Python dirver API in order to read and write from and to tables in parallel across the cluster. The datasource uses the DB API 2 `execute` and `executemany` methods to enable parallel read and writes of data.\n",
    "\n",
    "The connector is also a Python context manager, and utilize `with` semantics to define when a connection should be established, db operations commited to the database, and the connection closed. \n",
    "\n",
    "The code below will read from a sample table using the connector to manage the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from ray.data.datasource import DBAPI2Connector\n",
    "\n",
    "# query the number of rows, using the connection context to\n",
    "# manage transactions\n",
    "with DBAPI2Connector(connect_fn, **connect_props) as con:\n",
    "    count = con.query_int(f'SELECT COUNT(*) FROM source')\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use `try` blocks with the connector's `open`, `commit` and `close` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "connector = DBAPI2Connector(connect_fn, **connect_props)\n",
    "try:\n",
    "    connector.open()\n",
    "    count = connector.query_int(f'SELECT COUNT(*) FROM source')\n",
    "finally:\n",
    "    connector.close()\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB API 2 Datasource\n",
    "The DB API 2 datasource can be used with the Ray data `read_datasource` and `write_datasource` methods to read and write to databases using the distibuted processing capabilities of Ray data. The datasource uses the DBAPI2Connector class internally.\n",
    "\n",
    "Below is an exmaple of creating the datasource using the previously defined connect properties, and then using it to read and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 3/3 [00:00<00:00, 38.99it/s]\n",
      "Read progress: 100%|██████████| 3/3 [00:00<00:00, 2546.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_val</th>\n",
       "      <th>str_val</th>\n",
       "      <th>flt_val</th>\n",
       "      <th>bool_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_val str_val  flt_val  bool_val\n",
       "0        0       0      0.0         1\n",
       "1        1       1      1.0         0\n",
       "2        2       2      2.0         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data.datasource import DBAPI2Datasource\n",
    "from ray.data import read_datasource\n",
    "\n",
    "# create a datasource from a connector\n",
    "datasource = DBAPI2Datasource(connector)\n",
    "\n",
    "# use read_datasource to read\n",
    "ds = read_datasource(\n",
    "    datasource, \n",
    "    table='source'\n",
    ")\n",
    " \n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 15:42:49,416\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[read->write]\n",
      "read->write: 100%|██████████| 10/10 [00:02<00:00,  3.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# use write_datasource to write\n",
    "ds.write_datasource(datasource, table='destination')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DML and DDL\n",
    "The connector can also be used for any DDL or DML operations you would normally execute through the DB Native Python API. These operations just pass through to the underlying API. \n",
    "\n",
    "The code below will create the objects needed for writing to tables. Note that a commit is issued between the queries so the DDL operation executes prior to the next one that is dependent. An alternative is to use two `with` blocks to define transaction boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 15:43:56,912\tERROR database_datasource.py:94 -- error executing :\n",
      "  '\n",
      "          DROP TABLE destination;\n",
      "          DROP TABLE destination2;\n",
      "          DROP TABLE source\n",
      "      '\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py\", line 283, in execute\n",
      "    return self._execute(query, **kwargs)\n",
      "  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/dbapi2_datasource.py\", line 82, in _execute\n",
      "    cursor.execute(q, *query_args, **query_kwargs)\n",
      "sqlite3.OperationalError: no such table: destination\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "no such table: destination",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m connector \u001b[39mas\u001b[39;00m con:\n\u001b[0;32m----> 2\u001b[0m     con\u001b[39m.\u001b[39;49mquery(\u001b[39mf\u001b[39;49m\u001b[39m'''\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[39m        DROP TABLE destination;\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[39m        DROP TABLE destination2;\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[39m        DROP TABLE source\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39m'''\u001b[39;49m)\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py:257\u001b[0m, in \u001b[0;36mDatabaseConnector.query\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py:289\u001b[0m, in \u001b[0;36mDatabaseConnector.execute\u001b[0;34m(self, query, warn_on_error, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     _error(\u001b[39m'\u001b[39m\u001b[39merror executing \u001b[39m\u001b[39m'\u001b[39m, query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/database_datasource.py:283\u001b[0m, in \u001b[0;36mDatabaseConnector.execute\u001b[0;34m(self, query, warn_on_error, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_fn(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    282\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    284\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    285\u001b[0m     \u001b[39mif\u001b[39;00m warn_on_error:\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/datasource/dbapi2_datasource.py:82\u001b[0m, in \u001b[0;36mDBAPI2Connector._execute\u001b[0;34m(self, query, data, query_args, **query_kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     queries \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m     \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m queries:                \n\u001b[0;32m---> 82\u001b[0m         cursor\u001b[39m.\u001b[39;49mexecute(q, \u001b[39m*\u001b[39;49mquery_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mquery_kwargs)\n\u001b[1;32m     84\u001b[0m \u001b[39mreturn\u001b[39;00m cursor\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: destination"
     ]
    }
   ],
   "source": [
    "with connector as con:\n",
    "    con.query(f'''\n",
    "        DROP TABLE IF EXISTS destination;\n",
    "        DROP TABLE IF EXISTS  destination2;\n",
    "        DROP TABLE  IF EXISTS source\n",
    "    ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
