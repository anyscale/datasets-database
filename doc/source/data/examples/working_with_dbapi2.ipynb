{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Ray Data DB API 2 Connector](images/dbapi2_connector_logo.png)\n",
    "This user guide walks through the basics of reading and writing data with the Ray DB API 2 connector.\n",
    "\n",
    "The Ray DB API2 connector enables parallel read and write to and from SQL compliant datastores with Ray datasets. The connector utilizes the Python DB API 2.0 specification implemented by most databases. DB API 2 providers include:\n",
    "\n",
    "* SQLite 3 (this example)\n",
    "* Databricks (see [example](working_with_databricks.ipynb))\n",
    "* Snowflake (see [example](working_with_snowflake.ipynb))\n",
    "* Google Big Query\n",
    "* Microsoft SQL Server\n",
    "* Teradata\n",
    "* Postgres\n",
    "* MySQL\n",
    "* Oracle\n",
    "* Many more ...\n",
    "\n",
    "## Connection properties\n",
    "The databsae connection properties need to be provided to the data source upon creation. These properties are documented by the database provider, but for this  example we will use SQLite implementation which is provided in Python.\n",
    "\n",
    "Below is an example of loading properties from the environment, and filtering them by the 'SQLITE_' prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "env_connect_props = {\n",
    "    key.replace('SQLITE_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'SQLITE_' in key\n",
    "}\n",
    "\n",
    "if not os.path.exists('/mnt/cluster_storage/sqlite'):\n",
    "    os.makedirs('/mnt/cluster_storage/sqlite')\n",
    "\n",
    "# add db and schema in connect props\n",
    "connect_props = {\n",
    "    **env_connect_props,\n",
    "    'database':'/mnt/cluster_storage/sqlite/ray.db'\n",
    "}\n",
    "\n",
    "print('Connection properties:')\n",
    "print('\\n'.join(connect_props.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading\n",
    "Ray will use SQLite Python API to read in parallel into a Ray cluster. The created Ray datasets is composed of PyArrow dataframes that are spread across the Ray cluster to allow for the distributed operations required in machine learning.\n",
    "\n",
    "![DB API 2 read](images/dbapi2_read.png)\n",
    "\n",
    "\n",
    "### Read from tables\n",
    "In order to read an entire table into a a Ray cluster, utilize the Ray data `read_dbapi2` method. The code below will read in a sample table from an SQLite sample database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlite3 import connect as connect_fn\n",
    "from ray.data import read_dbapi2\n",
    "\n",
    "# create sample data\n",
    "SAMPLE_SIZE = 10\n",
    "with connect_fn(**connect_props) as con:\n",
    "    con.execute('DROP TABLE IF EXISTS source')\n",
    "    con.execute('CREATE TABLE source(int_val, str_val, flt_val, bool_val)')\n",
    "    con.commit()\n",
    "    data = [[int(i),str(i),float(i),i%2==0] for i in range(0,SAMPLE_SIZE)]\n",
    "    con.executemany('INSERT INTO source VALUES (?, ?, ?, ?)', data)\n",
    "\n",
    "# read the entire table\n",
    "ds = read_dbapi2(connect_fn, connect_props, table='source') \n",
    "\n",
    "# get all partitions\n",
    "ds = ds.fully_executed()\n",
    "\n",
    "# display the first 3 results\n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read with a query\n",
    "For more control over columns and rows read, as well as joining data from multiple tables, a query can be specified instead of a table name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = 'SELECT int_val, str_val FROM source WHERE int_val >= 3 AND int_val <= 5'\n",
    "\n",
    "# read the result of the query\n",
    "ds2 = read_dbapi2(connect_fn, connect_props, query=QUERY)\n",
    "\n",
    "# display the first 3 results\n",
    "ds2.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional read parameters\n",
    "For reading from Snowflake, underlying Python API arguments are also available. The `args` parameter will be passed to the underlying\n",
    "execute method. Other drivers may require kwargs whihc can be passed as key word arguments to the `read_dbapi2` method.\n",
    "\n",
    "The code below uses the args to specify parameterss to be used by SQLite when executing the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = 'SELECT int_val, str_val FROM source WHERE int_val > ?'\n",
    "\n",
    "ds3 = read_dbapi2(connect_fn, connect_props, query=QUERY, query_args=[[5]])\n",
    "ds3.limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing\n",
    "The Ray DB API 2 connector will use the DB API driver to write each partition of data in parallel. Each partition of data in the Ray dataset will have a write task that writes in parallel to Snowflake.\n",
    "![DB API 2 write](images/dbapi2_write.png)\n",
    "\n",
    "### Write to tables\n",
    "In order to write a dataset into database table, use the `write_dbapi2` method of the dataset object. Repartition the dataset prior to calling this method in order to set the number of write tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the destination table\n",
    "with connect_fn(**connect_props) as con:\n",
    "    con.execute('DROP TABLE IF EXISTS destination')\n",
    "    con.execute('CREATE TABLE destination(int_val, str_val, flt_val, bool_val)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below writes the previously read data into a new database table that are created using the Snowflake Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataset to the table \n",
    "ds.write_dbapi2(connect_fn, connect_props, table='destination')\n",
    "\n",
    "#read_dbapi2(connect_fn, connect_props, table='destination').limit(3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dbapi2(connect_fn, connect_props, table='destination').limit(3).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing with stage tables\n",
    "Some databases may lock tables during writing which would significantly increase the overal write time. To get around this, the write mode can be set to stage, wich will cause each partition to be writtent its own stage table. After all data is written to stage tables, they are then copied to the main table. In order for this to work, the database must support the defualt `CREATE TABLE <STAGE> LIKE <DESTINATION>` semantics. \n",
    "\n",
    "![DB API 2 write](images/dbapi2_write_staged.png)\n",
    "\n",
    "> Note: SQLite doesn't support `CREATE TABLE <STAGE> LIKE <DESTINATION>`. Since these semantics are not supported, the below overrides the prepare query to provide the template of sql for creating the stage table. `{table}` will be replaced with the destination table name, and `{block_id}` will be replaced with a unique id for the data block/partition. Many databases may support this, so the `write_queries` parameter would not be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataset to the table, using an autocreated table\n",
    "ds.write_dbapi2(\n",
    "    connect_fn,\n",
    "    connect_props, \n",
    "    table='destination',\n",
    "    mode='stage'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "If more low level access to the Ray DB API 2 connector is needed, the underlying `DBAPI2Connector` and `DBAPI2Datasource` can be used.\n",
    "\n",
    "### DB API 2 Connector\n",
    "The `DBAPI2Connector` class holds the connection properties and logic required to establish a connection with a DB API 2 driver for a database. Internally it calls the native Python dirver API in order to read and write from and to tables in parallel across the cluster. The datasource uses the DB API 2 `execute` and `executemany` methods to enable parallel read and writes of data.\n",
    "\n",
    "The connector is also a Python context manager, and utilize `with` semantics to define when a connection should be established, db operations commited to the database, and the connection closed. \n",
    "\n",
    "The code below will read from a sample table using the connector to manage the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import DBAPI2Connector\n",
    "\n",
    "# query the number of rows, using the connection context to\n",
    "# manage transactions\n",
    "with DBAPI2Connector(connect_fn, **connect_props) as con:\n",
    "    count = con.query_int(f'SELECT COUNT(*) FROM source')\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use `try` blocks with the connector's `open`, `commit` and `close` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = DBAPI2Connector(connect_fn, **connect_props)\n",
    "try:\n",
    "    connector.open()\n",
    "    count = connector.query_int(f'SELECT COUNT(*) FROM source')\n",
    "finally:\n",
    "    connector.close()\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB API 2 Datasource\n",
    "The DB API 2 datasource can be used with the Ray data `read_datasource` and `write_datasource` methods to read and write to databases using the distibuted processing capabilities of Ray data. The datasource uses the DBAPI2Connector class internally.\n",
    "\n",
    "Below is an exmaple of creating the datasource using the previously defined connect properties, and then using it to read and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import DBAPI2Datasource\n",
    "from ray.data import read_datasource\n",
    "\n",
    "# create a datasource from a connector\n",
    "datasource = DBAPI2Datasource(connector)\n",
    "\n",
    "# use read_datasource to read\n",
    "ds = read_datasource(\n",
    "    datasource, \n",
    "    table='source'\n",
    ")\n",
    " \n",
    "ds.limit(3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use write_datasource to write\n",
    "ds.write_datasource(datasource, table='destination')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DML and DDL\n",
    "The connector can also be used for any DDL or DML operations you would normally execute through the DB Native Python API. These operations just pass through to the underlying API. \n",
    "\n",
    "The code below will create the objects needed for writing to tables. Note that a commit is issued between the queries so the DDL operation executes prior to the next one that is dependent. An alternative is to use two `with` blocks to define transaction boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connector as con:\n",
    "    con.query(f'''\n",
    "        DROP TABLE destination;\n",
    "        DROP TABLE source\n",
    "    ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
