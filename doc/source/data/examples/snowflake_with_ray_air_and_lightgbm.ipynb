{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake with Ray Air\n",
    "By using the Ray Snowflake connector to read and write data into and out of Ray Datasets, all of the capabilities of Ray AIR can be used to build end to end machine learning applications. \n",
    "\n",
    "## Snowflake with Ray AIR and LightGBM\n",
    "For this example we will show how to train and tune a [distributed LightGBM](https://docs.ray.io/en/master/ray-air/examples/lightgbm_example.html) model with Ray AIR using Snowflake data. We will then show how to score data with the trained model and push the scored data back into another Snowflake table.\n",
    "\n",
    "### Set up the connector\n",
    "The first step is to get a dictionary of connection properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load from environment\n",
    "env_connect_props = {\n",
    "    key.replace('SNOWFLAKE_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'SNOWFLAKE_' in key\n",
    "}\n",
    "\n",
    "# add sample db and schema to connect props\n",
    "connect_props = {\n",
    "    **env_connect_props,\n",
    "    'database':'SNOWFLAKE_SAMPLE_DATA',\n",
    "    'schema':'TPCH_SF10',\n",
    "    'warehouse':'COMPUTE_WH'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Tuning\n",
    "A typical training or tuning workload will have the following logic when working with tabular data in Snowflake:\n",
    "\n",
    "![Ray Train with Snowflake](images/snowflake_train_with_air.png)\n",
    "\n",
    "#### Step 1: Stage data in Snowflake\n",
    "When working with databases, it is best to take advantage of native join and aggregation features of the database prior to ingesting data into Ray Datasets. Ray datasets is designed to power machine learning workflows, and does not provide some typical analytics capabilities like large joins. For these reasons, as a first step, the data required for training the model will be forumated into a single query that runs within Snowflake prior to reading with the Ray Snowflake connector. \n",
    "This query could also be materialized into a staging table\n",
    "if the data neesd to be used repeatedly.\n",
    "\n",
    " The code below creates a dataset of customer returns data from several Snowflake sample tables. We will use this data throughtout the train, tune and scoring process. In the code below, we use the `read_snowflake` method to read the data.\n",
    "\n",
    "> Note: The data set size is set to be small to keep execution times small. If you would like to try larger dataset size, increase the `SIZE` and be sure to have a large enough cluster defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = 'SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL'\n",
    "SIZE = 10000\n",
    "query = f\"\"\"\n",
    "    WITH cstmrs as (\n",
    "        SELECT \n",
    "            c_customer_sk as c_customer_sk, \n",
    "            c_current_cdemo_sk as c_current_cdemo_sk\n",
    "        FROM {SRC}.customer LIMIT {SIZE}),\n",
    "    sales as (\n",
    "        SELECT \n",
    "            c_customer_sk, \n",
    "            COUNT(c_customer_sk) as n_sales \n",
    "            FROM cstmrs JOIN {SRC}.store_sales ON c_customer_sk = ss_customer_sk\n",
    "        GROUP BY c_customer_sk),\n",
    "                    \n",
    "    rtrns as (\n",
    "        SELECT \n",
    "            c_customer_sk, \n",
    "            COUNT(c_customer_sk) as n_returns \n",
    "            FROM cstmrs JOIN {SRC}.store_returns ON c_customer_sk = sr_customer_sk\n",
    "        GROUP BY c_customer_sk)\n",
    "                        \n",
    "    SELECT\n",
    "        cstmrs.c_customer_sk as customer_sk,\n",
    "        ZEROIFNULL(n_sales) as n_sales,\n",
    "        ZEROIFNULL(n_returns) as n_returns,\n",
    "        IFF(n_sales is null or n_sales = 0 or n_returns is null, 0, n_returns/n_sales) as return_probability,\n",
    "        demos.* \n",
    "    FROM cstmrs \n",
    "    JOIN {SRC}.customer_demographics as demos ON cstmrs.c_current_cdemo_sk = demos.cd_demo_sk\n",
    "    LEFT OUTER JOIN sales on cstmrs.c_customer_sk = sales.c_customer_sk\n",
    "    LEFT OUTER JOIN rtrns on cstmrs.c_customer_sk = rtrns.c_customer_sk\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Read data into Ray\n",
    "Now that we have a qujery, we can read it into a Ray dataset. \n",
    "> Note: Only the first partition is read into the dataset. Whe training starts, additional partitions will be pulled into the dataset when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import read_snowflake\n",
    "ds = read_snowflake(connect_props, query=query)\n",
    "    \n",
    "ds.limit(10).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train\n",
    "Now that the data is read into a Ray dataset, we can use it to train or tune a LighGBM model. \n",
    "\n",
    "**Prepare the data**\n",
    "\n",
    "After reading the data, we need to do some simple manipulations to drop columns and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLUMNS = ['N_SALES', 'N_RETURNS', 'CD_DEMO_SK']\n",
    "\n",
    "ds = ds.drop_columns(DROP_COLUMNS).repartition(100)\n",
    "train_dataset, valid_dataset = ds.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create preprocessors**\n",
    "\n",
    "In Ray Air, all trainers, tuners and predcitors allow for the addition of preprocessors. Preprocessors help to featurize data, by providing common operations like on-hot-encoding, categorizing, scaling, etc. For more on the available preprocessors, read the [RayAIR docs](https://docs.ray.io/en/latest/ray-air/package-ref.html#preprocessor). The code below will use a chain of pre-processors. The `BatchMapper` will drop the ID column so it wont be used when training. The `Categorizer` will categorize columns, and the `StandardScaler` will scale columns. All of the pre-processing logic only modifes the data as it is being passed into training algorithms, and the underlying dataset will remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import Chain, BatchMapper, Categorizer, StandardScaler\n",
    "\n",
    "ID_COLUMN = 'CUSTOMER_SK'\n",
    "CATEGORICAL_COLUMNS = ['CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING']\n",
    "SCALAR_COLUMNS = ['CD_PURCHASE_ESTIMATE', 'CD_DEP_COUNT', 'CD_DEP_EMPLOYED_COUNT', 'CD_DEP_COLLEGE_COUNT']\n",
    "\n",
    "# Scale some random columns, and categorify the categorical_column,\n",
    "# allowing LightGBM to use its built-in categorical feature support\n",
    "preprocessor = Chain(\n",
    "    BatchMapper(lambda df: df.drop(ID_COLUMN, axis=1)),\n",
    "    Categorizer(CATEGORICAL_COLUMNS), \n",
    "    StandardScaler(columns=SCALAR_COLUMNS)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure scaling**\n",
    "\n",
    "Training requires compute infrastructure, and specifying what type is needed to optimize your training time and costs. When first beginning, it is best to start with a small dataset size and compute to get things working and then scale up data and compute together. Below we create a `ScalingConfig` that provides 10 workers for distributed trianing. This will likely keep training on a single instance. We also don't request GPU's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "scaling_config=ScalingConfig(num_workers=10, use_gpu=False),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a trainer**\n",
    "\n",
    "Now that we have everything required, we can create a trainer. In Ray AIR, the logic to create a trainer and fit it are very simliar. The main differences are in the parameters passed to the algorithm. This makes it easy to swap out algorithms. For example, swapping LightGBM for XGBoost, or even PyTorch tabular, will typically be just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightgbm import LightGBMTrainer\n",
    "\n",
    "TARGET_COLUMN = 'RETURN_PROBABILITY'\n",
    "\n",
    "# LightGBM specific params\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": [\"rmse\", \"mae\"],\n",
    "}\n",
    "\n",
    "trainer = LightGBMTrainer(\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=False),\n",
    "    label_column=TARGET_COLUMN,\n",
    "    params=params,\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    "    num_boost_round=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model**\n",
    "\n",
    "Now that the trainer is defined, al that is required is to call fit to begin the training process. The `fit` method will return a results object that containes the model checkpoint as well as model training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score a model\n",
    "Once there is a trained model, we can use it to score data. The flow for training and scoring are similar in that data is staged in Snowflake and read into a Ray dataset with the connector. Once the data is read in, the previously created model checkpoint can be used to creat a batch predictor for scoring. Scored data can then be written back into Snowflake with the connector. \n",
    "\n",
    "The typical logical flow for a batch scoring in Snowflake with Ray AIR is the following:\n",
    "\n",
    "![Snowflake batch scoring](images/snowflake_score_with_air.png)\n",
    "\n",
    "#### Steps 1-2: Stage and read data\n",
    "Since the data has already been staged and loaded, we dont need any extra code to do that now. Typically, you will have a script for training, and a script for scoring that will be run independently. The staging and loading of data should be sperated into a shared script that can be used by each of these workflows.\n",
    "\n",
    "#### Step 3: Score the data\n",
    "The previously trained checkpoint can be used to create a predictor. This predictor will already contain the pre-processors used to train the model. All that is needed is to drop the target column before feeding it into the model to simulate a real dataset where we dont know the results.\n",
    "\n",
    "> Note: Typically model checkpoints will be stored in a model registry provided by Weights and Biases or MLFlow, or into an objects store like S3. Checkpoints are written and read using the [checkpoint API](https://docs.ray.io/en/latest/ray-air/package-ref.html#ray.air.checkpoint.Checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.lightgbm import LightGBMPredictor\n",
    "\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    result.checkpoint, LightGBMPredictor\n",
    ")\n",
    "\n",
    "test_dataset = valid_dataset.drop_columns(TARGET_COLUMN)\n",
    "predictions = predictor.predict(test_dataset, keep_columns=[ID_COLUMN])\n",
    "predictions.limit(10).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Write data to Snowflake\n",
    "Now that we have the predictions we can write them into a Snowflake table. We need to first create a destination database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import SnowflakeConnector\n",
    "\n",
    "# get new connect proeprties for the new database\n",
    "write_connect_props = {\n",
    "    **connect_props, \n",
    "    'database':'RAY_SAMPLE', \n",
    "    'schema':'PUBLIC'\n",
    "}\n",
    "\n",
    "# create destination database\n",
    "with SnowflakeConnector(**write_connect_props) as con:\n",
    "    con.query(f'CREATE DATABASE IF NOT EXISTS RAY_SAMPLE')\n",
    "    \n",
    "\n",
    "# write the predictions\n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='PREDICTIONS',\n",
    "    autocreate=True\n",
    ")\n",
    "\n",
    "# read the predictions back\n",
    "read_snowflake(\n",
    "    write_connect_props, \n",
    "    table='PREDICTIONS'\n",
    ").limit(3).to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
