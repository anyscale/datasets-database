{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake with Ray Air\n",
    "By using the Ray Snowflake connector to read and write data into and out of Ray Datasets, all of the capabilities of Ray AIR can be used to build end to end machine learning applications. \n",
    "\n",
    "## Snowflake with Ray AIR and LightGBM\n",
    "For this example we will show how to train and tune a [distributed LightGBM](https://docs.ray.io/en/master/ray-air/examples/lightgbm_example.html) model with Ray AIR using Snowflake data. We will then show how to score data with the trained model and push the scored data back into another Snowflake table.\n",
    "\n",
    "### Set up the connector\n",
    "The first step is to get a dictionary of connection properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load from environment\n",
    "env_connect_props = {\n",
    "    key.replace('SNOWFLAKE_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'SNOWFLAKE_' in key\n",
    "}\n",
    "\n",
    "# add sample db and schema to connect props\n",
    "connect_props = {\n",
    "    **env_connect_props,\n",
    "    'database':'SNOWFLAKE_SAMPLE_DATA',\n",
    "    'schema':'TPCH_SF10',\n",
    "    'warehouse':'COMPUTE_WH'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Tuning\n",
    "A typical training or tuning workload will have the following logic when working with tabular data in Snowflake:\n",
    "\n",
    "![Ray Train with Snowflake](images/snowflake_train_with_air.png)\n",
    "\n",
    "#### Step 1: Stage data in Snowflake\n",
    "When working with databases, it is best to take advantage of native join and aggregation features of the database prior to ingesting data into Ray Datasets. Ray datasets is designed to power machine learning workflows, and does not provide some typical analytics capabilities like large joins. For these reasons, as a first step, the data required for training the model will be forumated into a single query that runs within Snowflake prior to reading with the Ray Snowflake connector. \n",
    "This query could also be materialized into a staging table\n",
    "if the data neesd to be used repeatedly.\n",
    "\n",
    " The code below creates a dataset of customer returns data from several Snowflake sample tables. We will use this data throughtout the train, tune and scoring process. In the code below, we use the `read_snowflake` method to read the data.\n",
    "\n",
    "> Note: The data set size is set to be small to keep execution times small. If you would like to try larger dataset size, increase the `SIZE` and be sure to have a large enough cluster defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = 'SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL'\n",
    "SIZE = 1000\n",
    "query = f\"\"\"\n",
    "    WITH cstmrs as (\n",
    "        SELECT \n",
    "            c_customer_sk as c_customer_sk, \n",
    "            c_current_cdemo_sk as c_current_cdemo_sk\n",
    "        FROM {SRC}.customer LIMIT {SIZE}),\n",
    "    sales as (\n",
    "        SELECT \n",
    "            c_customer_sk, \n",
    "            COUNT(c_customer_sk) as n_sales \n",
    "            FROM cstmrs JOIN {SRC}.store_sales ON c_customer_sk = ss_customer_sk\n",
    "        GROUP BY c_customer_sk),\n",
    "                    \n",
    "    rtrns as (\n",
    "        SELECT \n",
    "            c_customer_sk, \n",
    "            COUNT(c_customer_sk) as n_returns \n",
    "            FROM cstmrs JOIN {SRC}.store_returns ON c_customer_sk = sr_customer_sk\n",
    "        GROUP BY c_customer_sk)\n",
    "                        \n",
    "    SELECT\n",
    "        cstmrs.c_customer_sk as customer_sk,\n",
    "        ZEROIFNULL(n_sales) as n_sales,\n",
    "        ZEROIFNULL(n_returns) as n_returns,\n",
    "        IFF(n_sales is null or n_sales = 0 or n_returns is null, 0, n_returns/n_sales) as return_probability,\n",
    "        demos.* \n",
    "    FROM cstmrs \n",
    "    JOIN {SRC}.customer_demographics as demos ON cstmrs.c_current_cdemo_sk = demos.cd_demo_sk\n",
    "    LEFT OUTER JOIN sales on cstmrs.c_customer_sk = sales.c_customer_sk\n",
    "    LEFT OUTER JOIN rtrns on cstmrs.c_customer_sk = rtrns.c_customer_sk\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Read data into Ray\n",
    "Now that we have a qujery, we can read it into a Ray dataset. \n",
    "> Note: Only the first partition is read into the dataset. Whe training starts, additional partitions will be pulled into the dataset when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 17:01:48,293\tINFO worker.py:1242 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "find: ‘.git’: No such file or directory\n",
      "2023-02-13 17:01:48,573\tINFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.0.0.196:9031...\n",
      "2023-02-13 17:01:48,578\tINFO worker.py:1544 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_vnmb5jgl4z6q98h61dx25rccju/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-02-13 17:01:48,583\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_6097d7d48ea8ab842da11ccf2f0af9f4.zip' (0.79MiB) to Ray cluster...\n",
      "2023-02-13 17:01:48,593\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_6097d7d48ea8ab842da11ccf2f0af9f4.zip'.\n",
      "2023-02-13 17:01:53,976\tWARNING read_api.py:333 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 34.39it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 1124.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_SK</th>\n",
       "      <th>N_SALES</th>\n",
       "      <th>N_RETURNS</th>\n",
       "      <th>RETURN_PROBABILITY</th>\n",
       "      <th>CD_DEMO_SK</th>\n",
       "      <th>CD_GENDER</th>\n",
       "      <th>CD_MARITAL_STATUS</th>\n",
       "      <th>CD_EDUCATION_STATUS</th>\n",
       "      <th>CD_PURCHASE_ESTIMATE</th>\n",
       "      <th>CD_CREDIT_RATING</th>\n",
       "      <th>CD_DEP_COUNT</th>\n",
       "      <th>CD_DEP_EMPLOYED_COUNT</th>\n",
       "      <th>CD_DEP_COLLEGE_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11063183</td>\n",
       "      <td>402</td>\n",
       "      <td>51</td>\n",
       "      <td>0.126866</td>\n",
       "      <td>718589</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>2 yr Degree</td>\n",
       "      <td>3000</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12361894</td>\n",
       "      <td>370</td>\n",
       "      <td>46</td>\n",
       "      <td>0.124324</td>\n",
       "      <td>969956</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>2 yr Degree</td>\n",
       "      <td>8500</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11063159</td>\n",
       "      <td>366</td>\n",
       "      <td>28</td>\n",
       "      <td>0.076503</td>\n",
       "      <td>244699</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>4 yr Degree</td>\n",
       "      <td>8000</td>\n",
       "      <td>High Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11063217</td>\n",
       "      <td>389</td>\n",
       "      <td>40</td>\n",
       "      <td>0.102828</td>\n",
       "      <td>1157747</td>\n",
       "      <td>M</td>\n",
       "      <td>W</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>10000</td>\n",
       "      <td>High Risk</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11063722</td>\n",
       "      <td>392</td>\n",
       "      <td>52</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>1288460</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>2 yr Degree</td>\n",
       "      <td>3500</td>\n",
       "      <td>Good</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11063419</td>\n",
       "      <td>360</td>\n",
       "      <td>38</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>977178</td>\n",
       "      <td>F</td>\n",
       "      <td>W</td>\n",
       "      <td>4 yr Degree</td>\n",
       "      <td>10000</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11063765</td>\n",
       "      <td>393</td>\n",
       "      <td>42</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>759334</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>4 yr Degree</td>\n",
       "      <td>4000</td>\n",
       "      <td>High Risk</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11063626</td>\n",
       "      <td>367</td>\n",
       "      <td>33</td>\n",
       "      <td>0.089918</td>\n",
       "      <td>122075</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11063300</td>\n",
       "      <td>367</td>\n",
       "      <td>80</td>\n",
       "      <td>0.217984</td>\n",
       "      <td>250398</td>\n",
       "      <td>F</td>\n",
       "      <td>W</td>\n",
       "      <td>Primary</td>\n",
       "      <td>9000</td>\n",
       "      <td>High Risk</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11063339</td>\n",
       "      <td>371</td>\n",
       "      <td>51</td>\n",
       "      <td>0.137466</td>\n",
       "      <td>1072234</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>4 yr Degree</td>\n",
       "      <td>9000</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_SK  N_SALES  N_RETURNS RETURN_PROBABILITY  CD_DEMO_SK CD_GENDER  \\\n",
       "0     11063183      402         51           0.126866      718589         M   \n",
       "1     12361894      370         46           0.124324      969956         F   \n",
       "2     11063159      366         28           0.076503      244699         M   \n",
       "3     11063217      389         40           0.102828     1157747         M   \n",
       "4     11063722      392         52           0.132653     1288460         F   \n",
       "5     11063419      360         38           0.105556      977178         F   \n",
       "6     11063765      393         42           0.106870      759334         F   \n",
       "7     11063626      367         33           0.089918      122075         M   \n",
       "8     11063300      367         80           0.217984      250398         F   \n",
       "9     11063339      371         51           0.137466     1072234         F   \n",
       "\n",
       "  CD_MARITAL_STATUS CD_EDUCATION_STATUS  CD_PURCHASE_ESTIMATE  \\\n",
       "0                 U         2 yr Degree                  3000   \n",
       "1                 D         2 yr Degree                  8500   \n",
       "2                 U         4 yr Degree                  8000   \n",
       "3                 W           Secondary                 10000   \n",
       "4                 U         2 yr Degree                  3500   \n",
       "5                 W         4 yr Degree                 10000   \n",
       "6                 S         4 yr Degree                  4000   \n",
       "7                 D             Unknown                  2000   \n",
       "8                 W             Primary                  9000   \n",
       "9                 S         4 yr Degree                  9000   \n",
       "\n",
       "  CD_CREDIT_RATING  CD_DEP_COUNT  CD_DEP_EMPLOYED_COUNT  CD_DEP_COLLEGE_COUNT  \n",
       "0         Low Risk             2                      4                     2  \n",
       "1             Good             5                      3                     3  \n",
       "2        High Risk             1                      6                     0  \n",
       "3        High Risk             3                      1                     4  \n",
       "4             Good             6                      4                     4  \n",
       "5         Low Risk             6                      3                     3  \n",
       "6        High Risk             2                      5                     2  \n",
       "7          Unknown             0                      3                     0  \n",
       "8        High Risk             2                      6                     0  \n",
       "9         Low Risk             2                      6                     3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data import read_snowflake\n",
    "ds = read_snowflake(connect_props, query=query)\n",
    "    \n",
    "ds.limit(10).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train\n",
    "Now that the data is read into a Ray dataset, we can use it to train or tune a LighGBM model. \n",
    "\n",
    "##### Prepare the data\n",
    "After reading the data, we need to do some simple manipulations to drop columns and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLUMNS = ['N_SALES', 'N_RETURNS', 'CD_DEMO_SK']\n",
    "\n",
    "ds = ds.fully_executed().drop_columns(DROP_COLUMNS).repartition(100)\n",
    "train_dataset, valid_dataset = ds.train_test_split(test_size=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create preprocessors\n",
    "In Ray Air, all trainers, tuners and predcitors allow for the addition of preprocessors. Preprocessors help to featurize data, by providing common operations like on-hot-encoding, categorizing, scaling, etc. For more on the available preprocessors, read the [RayAIR docs](https://docs.ray.io/en/latest/ray-air/package-ref.html#preprocessor). The code below will use a chain of pre-processors. The `BatchMapper` will drop the ID column so it wont be used when training. The `Categorizer` will categorize columns, and the `StandardScaler` will scale columns. All of the pre-processing logic only modifes the data as it is being passed into training algorithms, and the underlying dataset will remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import Chain, BatchMapper, Categorizer, StandardScaler\n",
    "\n",
    "ID_COLUMN = 'CUSTOMER_SK'\n",
    "CATEGORICAL_COLUMNS = ['CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING']\n",
    "SCALAR_COLUMNS = ['CD_PURCHASE_ESTIMATE', 'CD_DEP_COUNT', 'CD_DEP_EMPLOYED_COUNT', 'CD_DEP_COLLEGE_COUNT']\n",
    "\n",
    "# Scale some random columns, and categorify the categorical_column,\n",
    "# allowing LightGBM to use its built-in categorical feature support\n",
    "preprocessor = Chain(\n",
    "    BatchMapper(lambda df: df.drop(ID_COLUMN, axis=1), 'pandas'),\n",
    "    Categorizer(CATEGORICAL_COLUMNS), \n",
    "    StandardScaler(columns=SCALAR_COLUMNS)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure scaling\n",
    "Training requires compute infrastructure, and specifying what type is needed to optimize your training time and costs. When first beginning, it is best to start with a small dataset size and compute to get things working and then scale up data and compute together. Below we create a `ScalingConfig` that provides 10 workers for distributed trianing. This will likely keep training on a single instance. We also don't request GPU's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "scaling_config=ScalingConfig(num_workers=10, use_gpu=False),"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a trainer\n",
    "Now that we have everything required, we can create a trainer. In Ray AIR, the logic to create a trainer and fit it are very simliar. The main differences are in the parameters passed to the algorithm. This makes it easy to swap out algorithms. For example, swapping LightGBM for XGBoost, or even PyTorch tabular, will typically be just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightgbm import LightGBMTrainer\n",
    "\n",
    "TARGET_COLUMN = 'RETURN_PROBABILITY'\n",
    "\n",
    "# LightGBM specific params\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": [\"rmse\", \"mae\"],\n",
    "}\n",
    "\n",
    "trainer = LightGBMTrainer(\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=False),\n",
    "    label_column=TARGET_COLUMN,\n",
    "    params=params,\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    "    num_boost_round=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model\n",
    "Now that the trainer is defined, al that is required is to call fit to begin the training process. The `fit` method will return a results object that containes the model checkpoint as well as model training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(autoscaler +1m16s) Adding 4 node(s) of type worker-node-type-0.\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score a model\n",
    "Once there is a trained model, we can use it to score data. The flow for training and scoring are similar in that data is staged in Snowflake and read into a Ray dataset with the connector. Once the data is read in, the previously created model checkpoint can be used to creat a batch predictor for scoring. Scored data can then be written back into Snowflake with the connector. \n",
    "\n",
    "The typical logical flow for a batch scoring in Snowflake with Ray AIR is the following:\n",
    "\n",
    "![Snowflake batch scoring](images/snowflake_score_with_air.png)\n",
    "\n",
    "#### Steps 1-2: Stage and read data\n",
    "Since the data has already been staged and loaded, we dont need any extra code to do that now. Typically, you will have a script for training, and a script for scoring that will be run independently. The staging and loading of data should be sperated into a shared script that can be used by each of these workflows.\n",
    "\n",
    "#### Step 3: Score the data\n",
    "The previously trained checkpoint can be used to create a predictor. This predictor will already contain the pre-processors used to train the model. All that is needed is to drop the target column before feeding it into the model to simulate a real dataset where we dont know the results.\n",
    "\n",
    "> Note: Typically model checkpoints will be stored in a model registry provided by Weights and Biases or MLFlow, or into an objects store like S3. Checkpoints are written and read using the [checkpoint API](https://docs.ray.io/en/latest/ray-air/package-ref.html#ray.air.checkpoint.Checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.lightgbm import LightGBMPredictor\n",
    "\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    result.checkpoint, LightGBMPredictor\n",
    ")\n",
    "\n",
    "test_dataset = valid_dataset.drop_columns(TARGET_COLUMN)\n",
    "predictions = predictor.predict(test_dataset, keep_columns=[ID_COLUMN])\n",
    "predictions.limit(10).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Write data to Snowflake\n",
    "Now that we have the predictions we can write them into a Snowflake table. We need to first create a destination database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import SnowflakeConnector\n",
    "\n",
    "# get new connect proeprties for the new database\n",
    "write_connect_props = {\n",
    "    **connect_props, \n",
    "    'database':'RAY_SAMPLE', \n",
    "    'schema':'PUBLIC'\n",
    "}\n",
    "\n",
    "# create destination database\n",
    "with SnowflakeConnector(**write_connect_props) as con:\n",
    "    con.query(f'CREATE DATABASE IF NOT EXISTS RAY_SAMPLE')\n",
    "    \n",
    "\n",
    "# write the predictions\n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='PREDICTIONS',\n",
    "    autocreate=True\n",
    ")\n",
    "\n",
    "# read the predictions back\n",
    "read_snowflake(\n",
    "    write_connect_props, \n",
    "    table='PREDICTIONS'\n",
    ").limit(3).to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
