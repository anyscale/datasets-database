{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake with Ray Air\n",
    "By using the Ray Snowflake connector to read and write data into and out of Ray Datasets, all of the capabilities of Ray AIR can be used to build end to end machine learning applications. \n",
    "\n",
    "## Snowflake with Ray AIR and LightGBM\n",
    "For this example we will show how to train and tune a [distributed LightGBM](https://docs.ray.io/en/master/ray-air/examples/lightgbm_example.html) model with Ray AIR using Snowflake data. We will then show how to score data with the trained model and push the scored data back into another Snowflake table.\n",
    "\n",
    "### Set up the connector\n",
    "The first step is to get a dictionary of connection properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load from environment\n",
    "env_connect_props = {\n",
    "    key.replace('SNOWFLAKE_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'SNOWFLAKE_' in key\n",
    "}\n",
    "\n",
    "# add sample db and schema to connect props\n",
    "connect_props = {\n",
    "    **env_connect_props,\n",
    "    'database':'SNOWFLAKE_SAMPLE_DATA',\n",
    "    'schema':'TPCH_SF10',\n",
    "    'warehouse':'COMPUTE_WH'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Tuning\n",
    "A typical training or tuning workload will have the following logic when working with tabular data in Snowflake:\n",
    "\n",
    "![Ray Train with Snowflake](../images/database/snowflake_train_with_air.png)\n",
    "\n",
    "#### Step 1: Stage data in Snowflake\n",
    "When working with databases, it is best to take advantage of native join and aggregation features of the database prior to ingesting data into Ray Datasets. Ray datasets is designed to power machine learning workflows, and does not provide some typical analytics capabilities like large joins. For these reasons, as a first step, the data required for training the model will be forumated into a single query that runs within Snowflake prior to reading with the Ray Snowflake connector. \n",
    "This query could also be materialized into a staging table\n",
    "if the data neesd to be used repeatedly.\n",
    "\n",
    " The code below creates a dataset of customer returns data from several Snowflake sample tables. We will use this data throughtout the train, tune and scoring process. In the code below, we use the `read_snowflake` method to read the data.\n",
    "\n",
    "> Note: The data set size is set to be small to keep execution times small. If you would like to try larger dataset size, increase the `SIZE` and be sure to have a large enough cluster defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = 'SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL'\n",
    "SIZE = 1000\n",
    "query = f\"\"\"\n",
    "    WITH cstmrs as (\n",
    "        SELECT \n",
    "            c_customer_sk as c_customer_sk, \n",
    "            c_current_cdemo_sk as c_current_cdemo_sk\n",
    "        FROM {SRC}.customer LIMIT {SIZE}),\n",
    "    sales as (\n",
    "        SELECT \n",
    "            c_customer_sk, \n",
    "            COUNT(c_customer_sk) as n_sales \n",
    "            FROM cstmrs JOIN {SRC}.store_sales ON c_customer_sk = ss_customer_sk\n",
    "        GROUP BY c_customer_sk),\n",
    "                    \n",
    "    rtrns as (\n",
    "        SELECT \n",
    "            c_customer_sk, \n",
    "            COUNT(c_customer_sk) as n_returns \n",
    "            FROM cstmrs JOIN {SRC}.store_returns ON c_customer_sk = sr_customer_sk\n",
    "        GROUP BY c_customer_sk)\n",
    "                        \n",
    "    SELECT\n",
    "        cstmrs.c_customer_sk as customer_sk,\n",
    "        ZEROIFNULL(n_sales) as n_sales,\n",
    "        ZEROIFNULL(n_returns) as n_returns,\n",
    "        IFF(n_sales is null or n_sales = 0 or n_returns is null, 0, n_returns/n_sales) as return_probability,\n",
    "        demos.* \n",
    "    FROM cstmrs \n",
    "    JOIN {SRC}.customer_demographics as demos ON cstmrs.c_current_cdemo_sk = demos.cd_demo_sk\n",
    "    LEFT OUTER JOIN sales on cstmrs.c_customer_sk = sales.c_customer_sk\n",
    "    LEFT OUTER JOIN rtrns on cstmrs.c_customer_sk = rtrns.c_customer_sk\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Read data into Ray\n",
    "Now that we have a qujery, we can read it into a Ray dataset. \n",
    "> Note: Only the first partition is read into the dataset. Whe training starts, additional partitions will be pulled into the dataset when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 22:52:46,430\tINFO worker.py:1242 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "find: ‘.git’: No such file or directory\n",
      "2023-02-26 22:52:46,697\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.36.75:9031...\n",
      "2023-02-26 22:52:46,702\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_vnmb5jgl4z6q98h61dx25rccju/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-02-26 22:52:46,707\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_32e160b13109b4e1fa59327e76ad400a.zip' (0.85MiB) to Ray cluster...\n",
      "2023-02-26 22:52:46,719\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_32e160b13109b4e1fa59327e76ad400a.zip'.\n",
      "2023-02-26 22:52:51,799\tWARNING read_api.py:340 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_SK</th>\n",
       "      <th>N_SALES</th>\n",
       "      <th>N_RETURNS</th>\n",
       "      <th>RETURN_PROBABILITY</th>\n",
       "      <th>CD_DEMO_SK</th>\n",
       "      <th>CD_GENDER</th>\n",
       "      <th>CD_MARITAL_STATUS</th>\n",
       "      <th>CD_EDUCATION_STATUS</th>\n",
       "      <th>CD_PURCHASE_ESTIMATE</th>\n",
       "      <th>CD_CREDIT_RATING</th>\n",
       "      <th>CD_DEP_COUNT</th>\n",
       "      <th>CD_DEP_EMPLOYED_COUNT</th>\n",
       "      <th>CD_DEP_COLLEGE_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11063183</td>\n",
       "      <td>402</td>\n",
       "      <td>51</td>\n",
       "      <td>0.126866</td>\n",
       "      <td>718589</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>2 yr Degree</td>\n",
       "      <td>3000</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12594543</td>\n",
       "      <td>366</td>\n",
       "      <td>79</td>\n",
       "      <td>0.215847</td>\n",
       "      <td>13888</td>\n",
       "      <td>F</td>\n",
       "      <td>W</td>\n",
       "      <td>College</td>\n",
       "      <td>9500</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12594142</td>\n",
       "      <td>386</td>\n",
       "      <td>29</td>\n",
       "      <td>0.075130</td>\n",
       "      <td>1800112</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>8000</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11063280</td>\n",
       "      <td>390</td>\n",
       "      <td>36</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>1098337</td>\n",
       "      <td>M</td>\n",
       "      <td>W</td>\n",
       "      <td>2 yr Degree</td>\n",
       "      <td>5500</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11063268</td>\n",
       "      <td>391</td>\n",
       "      <td>24</td>\n",
       "      <td>0.061381</td>\n",
       "      <td>43826</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>Primary</td>\n",
       "      <td>3500</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12594514</td>\n",
       "      <td>380</td>\n",
       "      <td>27</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>1808411</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>2 yr Degree</td>\n",
       "      <td>7500</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11063217</td>\n",
       "      <td>389</td>\n",
       "      <td>40</td>\n",
       "      <td>0.102828</td>\n",
       "      <td>1157747</td>\n",
       "      <td>M</td>\n",
       "      <td>W</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>10000</td>\n",
       "      <td>High Risk</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12594665</td>\n",
       "      <td>377</td>\n",
       "      <td>34</td>\n",
       "      <td>0.090186</td>\n",
       "      <td>885643</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>Primary</td>\n",
       "      <td>6500</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12594173</td>\n",
       "      <td>367</td>\n",
       "      <td>41</td>\n",
       "      <td>0.111717</td>\n",
       "      <td>316290</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>College</td>\n",
       "      <td>9500</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12594723</td>\n",
       "      <td>394</td>\n",
       "      <td>37</td>\n",
       "      <td>0.093909</td>\n",
       "      <td>1323589</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>College</td>\n",
       "      <td>4500</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_SK  N_SALES  N_RETURNS RETURN_PROBABILITY  CD_DEMO_SK CD_GENDER  \\\n",
       "0     11063183      402         51           0.126866      718589         M   \n",
       "1     12594543      366         79           0.215847       13888         F   \n",
       "2     12594142      386         29           0.075130     1800112         F   \n",
       "3     11063280      390         36           0.092308     1098337         M   \n",
       "4     11063268      391         24           0.061381       43826         F   \n",
       "5     12594514      380         27           0.071053     1808411         M   \n",
       "6     11063217      389         40           0.102828     1157747         M   \n",
       "7     12594665      377         34           0.090186      885643         M   \n",
       "8     12594173      367         41           0.111717      316290         F   \n",
       "9     12594723      394         37           0.093909     1323589         M   \n",
       "\n",
       "  CD_MARITAL_STATUS CD_EDUCATION_STATUS  CD_PURCHASE_ESTIMATE  \\\n",
       "0                 U         2 yr Degree                  3000   \n",
       "1                 W             College                  9500   \n",
       "2                 M             Unknown                  8000   \n",
       "3                 W         2 yr Degree                  5500   \n",
       "4                 D             Primary                  3500   \n",
       "5                 M         2 yr Degree                  7500   \n",
       "6                 W           Secondary                 10000   \n",
       "7                 S             Primary                  6500   \n",
       "8                 U             College                  9500   \n",
       "9                 U             College                  4500   \n",
       "\n",
       "  CD_CREDIT_RATING  CD_DEP_COUNT  CD_DEP_EMPLOYED_COUNT  CD_DEP_COLLEGE_COUNT  \n",
       "0         Low Risk             2                      4                     2  \n",
       "1         Low Risk             2                      0                     0  \n",
       "2         Low Risk             6                      3                     6  \n",
       "3             Good             0                      0                     4  \n",
       "4          Unknown             0                      1                     0  \n",
       "5          Unknown             0                      4                     6  \n",
       "6        High Risk             3                      1                     4  \n",
       "7             Good             4                      1                     3  \n",
       "8         Low Risk             0                      1                     1  \n",
       "9         Low Risk             5                      5                     4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data import read_snowflake\n",
    "ds = read_snowflake(connect_props, query=query)\n",
    "    \n",
    "ds.limit(10).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train\n",
    "Now that the data is read into a Ray dataset, we can use it to train or tune a LighGBM model. \n",
    "\n",
    "##### Prepare the data\n",
    "After reading the data, we need to do some simple manipulations to drop columns and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 22:52:52,134\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[read->MapBatches(<lambda>)] -> AllToAllOperator[repartition]\n",
      "read->MapBatches(<lambda>): 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]\n",
      "Repartition:   0%|          | 0/100 [00:00<?, ?it/s]2023-02-26 22:52:52,593\tWARNING plan.py:528 -- Warning: The Ray cluster currently does not have any available CPUs. The Dataset job will hang unless more CPUs are freed up. A common reason is that cluster resources are used by Actors or Tune trials; see the following link for more details: https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune\n",
      "Repartition: 100%|██████████| 100/100 [00:00<00:00, 302.11it/s]\n"
     ]
    }
   ],
   "source": [
    "DROP_COLUMNS = ['N_SALES', 'N_RETURNS', 'CD_DEMO_SK']\n",
    "\n",
    "ds = ds.drop_columns(DROP_COLUMNS).repartition(100)\n",
    "train_dataset, valid_dataset = ds.train_test_split(test_size=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create preprocessors\n",
    "In Ray Air, all trainers, tuners and predcitors allow for the addition of preprocessors. Preprocessors help to featurize data, by providing common operations like on-hot-encoding, categorizing, scaling, etc. For more on the available preprocessors, read the [RayAIR docs](https://docs.ray.io/en/latest/ray-air/package-ref.html#preprocessor). The code below will use a chain of pre-processors. The `BatchMapper` will drop the ID column so it wont be used when training. The `Categorizer` will categorize columns, and the `StandardScaler` will scale columns. All of the pre-processing logic only modifes the data as it is being passed into training algorithms, and the underlying dataset will remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import Chain, BatchMapper, Categorizer, StandardScaler\n",
    "\n",
    "ID_COLUMN = 'CUSTOMER_SK'\n",
    "CATEGORICAL_COLUMNS = ['CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING']\n",
    "SCALAR_COLUMNS = ['CD_PURCHASE_ESTIMATE', 'CD_DEP_COUNT', 'CD_DEP_EMPLOYED_COUNT', 'CD_DEP_COLLEGE_COUNT']\n",
    "\n",
    "# Scale some random columns, and categorify the categorical_column,\n",
    "# allowing LightGBM to use its built-in categorical feature support\n",
    "preprocessor = Chain(\n",
    "    BatchMapper(lambda df: df.drop(ID_COLUMN, axis=1), 'pandas'),\n",
    "    Categorizer(CATEGORICAL_COLUMNS), \n",
    "    StandardScaler(columns=SCALAR_COLUMNS)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure scaling\n",
    "Training requires compute infrastructure, and specifying what type is needed to optimize your training time and costs. When first beginning, it is best to start with a small dataset size and compute to get things working and then scale up data and compute together. Below we create a `ScalingConfig` that provides 10 workers for distributed trianing. This will likely keep training on a single instance. We also don't request GPU's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "scaling_config=ScalingConfig(num_workers=10, use_gpu=False),"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a trainer\n",
    "Now that we have everything required, we can create a trainer. In Ray AIR, the logic to create a trainer and fit it are very simliar. The main differences are in the parameters passed to the algorithm. This makes it easy to swap out algorithms. For example, swapping LightGBM for XGBoost, or even PyTorch tabular, will typically be just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightgbm import LightGBMTrainer\n",
    "\n",
    "TARGET_COLUMN = 'RETURN_PROBABILITY'\n",
    "\n",
    "# LightGBM specific params\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": [\"rmse\", \"mae\"],\n",
    "}\n",
    "\n",
    "trainer = LightGBMTrainer(\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=False),\n",
    "    label_column=TARGET_COLUMN,\n",
    "    params=params,\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    "    num_boost_round=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model\n",
    "Now that the trainer is defined, al that is required is to call fit to begin the training process. The `fit` method will return a results object that containes the model checkpoint as well as model training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-26 22:53:07</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:12.22        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.6/30.8 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/16.92 GiB heap, 0.0/8.46 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightGBMTrainer_5c441_00000</td><td style=\"text-align: right;\">           1</td><td>/home/ray/ray_results/LightGBMTrainer_2023-02-26_22-52-54/LightGBMTrainer_5c441_00000_0_2023-02-26_22-52-55/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightGBMTrainer_5c441_00000</td><td>ERROR   </td><td>10.0.36.75:20778</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 22:53:07,746\tERROR tune.py:821 -- Trials did not complete: [LightGBMTrainer_5c441_00000]\n",
      "2023-02-26 22:53:07,746\tINFO tune.py:825 -- Total run time: 13.34 seconds (12.22 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "RayTaskError(RuntimeError)",
     "evalue": "\u001b[36mray::_Inner.train()\u001b[39m (pid=20778, ip=10.0.36.75, repr=LightGBMTrainer)\nray.exceptions.RayTaskError(RayXGBoostTrainingError): \u001b[36mray::_RemoteRayLightGBMActor.train()\u001b[39m (pid=21054, ip=10.0.36.75, repr=<lightgbm_ray.main._RemoteRayLightGBMActor object at 0x7f2dd50a49a0>)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/lightgbm_ray/main.py\", line 473, in train\n    raise RayXGBoostTrainingError(\"Training failed.\") from raise_from\nxgboost_ray.main.RayXGBoostTrainingError: Training failed.\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::_Inner.train()\u001b[39m (pid=20778, ip=10.0.36.75, repr=LightGBMTrainer)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/lightgbm_ray/main.py\", line 1248, in train\n    bst, train_evals_result, train_additional_results = _train(\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/lightgbm_ray/main.py\", line 846, in _train\n    raise RayActorError from exc\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::_Inner.train()\u001b[39m (pid=20778, ip=10.0.36.75, repr=LightGBMTrainer)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 360, in train\n    raise skipped from exception_cause(skipped)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n    return self._trainable_func(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 737, in _trainable_func\n    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n    output = fn()\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 647, in train_func\n    trainer.training_loop()\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/train/gbdt_trainer.py\", line 300, in training_loop\n    model = self._train(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/train/lightgbm/lightgbm_trainer.py\", line 92, in _train\n    return lightgbm_ray.train(**kwargs)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/lightgbm_ray/main.py\", line 1330, in train\n    raise RuntimeError(\nRuntimeError: A Ray actor died during training and the maximum number of retries (0) is exhausted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/train/base_trainer.py:579\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m     result \u001b[39m=\u001b[39m result_grid[\u001b[39m0\u001b[39m]\n\u001b[1;32m    578\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39merror:\n\u001b[0;32m--> 579\u001b[0m         \u001b[39mraise\u001b[39;00m result\u001b[39m.\u001b[39merror\n\u001b[1;32m    580\u001b[0m \u001b[39mexcept\u001b[39;00m TuneError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    581\u001b[0m     \u001b[39mraise\u001b[39;00m TrainingFailedError \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=20778, ip=10.0.36.75, repr=LightGBMTrainer)\nray.exceptions.RayTaskError(RayXGBoostTrainingError): \u001b[36mray::_RemoteRayLightGBMActor.train()\u001b[39m (pid=21054, ip=10.0.36.75, repr=<lightgbm_ray.main._RemoteRayLightGBMActor object at 0x7f2dd50a49a0>)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/lightgbm_ray/main.py\", line 473, in train\n    raise RayXGBoostTrainingError(\"Training failed.\") from raise_from\nxgboost_ray.main.RayXGBoostTrainingError: Training failed.\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::_Inner.train()\u001b[39m (pid=20778, ip=10.0.36.75, repr=LightGBMTrainer)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/lightgbm_ray/main.py\", line 1248, in train\n    bst, train_evals_result, train_additional_results = _train(\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/lightgbm_ray/main.py\", line 846, in _train\n    raise RayActorError from exc\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::_Inner.train()\u001b[39m (pid=20778, ip=10.0.36.75, repr=LightGBMTrainer)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 360, in train\n    raise skipped from exception_cause(skipped)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n    return self._trainable_func(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 737, in _trainable_func\n    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n    output = fn()\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/train/base_trainer.py\", line 647, in train_func\n    trainer.training_loop()\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/train/gbdt_trainer.py\", line 300, in training_loop\n    model = self._train(\n  File \"/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/train/lightgbm/lightgbm_trainer.py\", line 92, in _train\n    return lightgbm_ray.train(**kwargs)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/lightgbm_ray/main.py\", line 1330, in train\n    raise RuntimeError(\nRuntimeError: A Ray actor died during training and the maximum number of retries (0) is exhausted."
     ]
    }
   ],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score a model\n",
    "Once there is a trained model, we can use it to score data. The flow for training and scoring are similar in that data is staged in Snowflake and read into a Ray dataset with the connector. Once the data is read in, the previously created model checkpoint can be used to creat a batch predictor for scoring. Scored data can then be written back into Snowflake with the connector. \n",
    "\n",
    "The typical logical flow for a batch scoring in Snowflake with Ray AIR is the following:\n",
    "\n",
    "![Snowflake batch scoring](../images/database/snowflake_score_with_air.png)\n",
    "\n",
    "#### Steps 1-2: Stage and read data\n",
    "Since the data has already been staged and loaded, we dont need any extra code to do that now. Typically, you will have a script for training, and a script for scoring that will be run independently. The staging and loading of data should be sperated into a shared script that can be used by each of these workflows.\n",
    "\n",
    "#### Step 3: Score the data\n",
    "The previously trained checkpoint can be used to create a predictor. This predictor will already contain the pre-processors used to train the model. All that is needed is to drop the target column before feeding it into the model to simulate a real dataset where we dont know the results.\n",
    "\n",
    "> Note: Typically model checkpoints will be stored in a model registry provided by Weights and Biases or MLFlow, or into an objects store like S3. Checkpoints are written and read using the [checkpoint API](https://docs.ray.io/en/latest/ray-air/package-ref.html#ray.air.checkpoint.Checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.lightgbm import LightGBMPredictor\n",
    "\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    result.checkpoint, LightGBMPredictor\n",
    ")\n",
    "\n",
    "test_dataset = valid_dataset.drop_columns(TARGET_COLUMN)\n",
    "predictions = predictor.predict(test_dataset, keep_columns=[ID_COLUMN])\n",
    "predictions.limit(10).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Write data to Snowflake\n",
    "Now that we have the predictions we can write them into a Snowflake table. We need to first create a destination database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import SnowflakeConnector\n",
    "\n",
    "# get new connect proeprties for the new database\n",
    "write_connect_props = {\n",
    "    **connect_props, \n",
    "    'database':'RAY_SAMPLE', \n",
    "    'schema':'PUBLIC'\n",
    "}\n",
    "\n",
    "# create destination database\n",
    "with SnowflakeConnector(**write_connect_props) as con:\n",
    "    con.query(f'CREATE DATABASE IF NOT EXISTS RAY_SAMPLE')\n",
    "    \n",
    "\n",
    "# write the predictions\n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='PREDICTIONS',\n",
    "    autocreate=True\n",
    ")\n",
    "\n",
    "# read the predictions back\n",
    "read_snowflake(\n",
    "    write_connect_props, \n",
    "    table='PREDICTIONS'\n",
    ").limit(3).to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
