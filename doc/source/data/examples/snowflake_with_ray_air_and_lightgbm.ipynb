{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake with Ray Air\n",
    "By using the Ray Snowflake connector to read and write data into and out of Ray Datasets, all of the capabilities of Ray AIR can be used to build end to end machine learning applications. \n",
    "\n",
    "## Snowflake with Ray AIR and LightGBM\n",
    "For this example we will show how to train and tune a [distributed LightGBM](https://docs.ray.io/en/master/ray-air/examples/lightgbm_example.html) model with Ray AIR using Snowflake data. We will then show how to score data with the trained model and push the scored data back into another Snowflake table.\n",
    "\n",
    "### Set up the connector\n",
    "The first step is to get a dictionary of connection properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load from environment\n",
    "env_connect_props = {\n",
    "    key.replace('SNOWFLAKE_','').lower(): value \n",
    "    for key,value in os.environ.items() if 'SNOWFLAKE_' in key\n",
    "}\n",
    "\n",
    "# add sample db and schema to connect props\n",
    "connect_props = {\n",
    "    **env_connect_props,\n",
    "    'database':'SNOWFLAKE_SAMPLE_DATA',\n",
    "    'schema':'TPCH_SF10',\n",
    "    'warehouse':'COMPUTE_WH'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Tuning\n",
    "A typical training or tuning workload will have the following logic when working with tabular data in Snowflake:\n",
    "\n",
    "![Ray Train with Snowflake](images/snowflake_train_with_air.png)\n",
    "\n",
    "#### Step 1: Stage data in Snowflake\n",
    "When working with databases, it is best to take advantage of native join and aggregation features of the database prior to ingesting data into Ray Datasets. Ray datasets is designed to power machine learning workflows, and does not provide some typical analytics capabilities like large joins. For these reasons, as a first step, the data required for training the model will be forumated into a single query that runs within Snowflake prior to reading with the Ray Snowflake connector. \n",
    "This query could also be materialized into a staging table\n",
    "if the data neesd to be used repeatedly.\n",
    "\n",
    " The code below creates a dataset of customer returns data from several Snowflake sample tables. We will use this data throughtout the train, tune and scoring process. In the code below, we use the `read_snowflake` method to read the data.\n",
    "\n",
    "> Note: The data set size is set to be small to keep execution times small. If you would like to try larger dataset size, increase the `SIZE` and be sure to have a large enough cluster defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = 'SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL'\n",
    "SIZE = 1000\n",
    "query = f\"\"\"\n",
    "    WITH cstmrs as (\n",
    "        SELECT \n",
    "            c_customer_sk as c_customer_sk, \n",
    "            c_current_cdemo_sk as c_current_cdemo_sk\n",
    "        FROM {SRC}.customer LIMIT {SIZE}),\n",
    "    sales as (\n",
    "        SELECT \n",
    "            c_customer_sk, \n",
    "            COUNT(c_customer_sk) as n_sales \n",
    "            FROM cstmrs JOIN {SRC}.store_sales ON c_customer_sk = ss_customer_sk\n",
    "        GROUP BY c_customer_sk),\n",
    "                    \n",
    "    rtrns as (\n",
    "        SELECT \n",
    "            c_customer_sk, \n",
    "            COUNT(c_customer_sk) as n_returns \n",
    "            FROM cstmrs JOIN {SRC}.store_returns ON c_customer_sk = sr_customer_sk\n",
    "        GROUP BY c_customer_sk)\n",
    "                        \n",
    "    SELECT\n",
    "        cstmrs.c_customer_sk as customer_sk,\n",
    "        ZEROIFNULL(n_sales) as n_sales,\n",
    "        ZEROIFNULL(n_returns) as n_returns,\n",
    "        IFF(n_sales is null or n_sales = 0 or n_returns is null, 0, n_returns/n_sales) as return_probability,\n",
    "        demos.* \n",
    "    FROM cstmrs \n",
    "    JOIN {SRC}.customer_demographics as demos ON cstmrs.c_current_cdemo_sk = demos.cd_demo_sk\n",
    "    LEFT OUTER JOIN sales on cstmrs.c_customer_sk = sales.c_customer_sk\n",
    "    LEFT OUTER JOIN rtrns on cstmrs.c_customer_sk = rtrns.c_customer_sk\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Read data into Ray\n",
    "Now that we have a qujery, we can read it into a Ray dataset. \n",
    "> Note: Only the first partition is read into the dataset. Whe training starts, additional partitions will be pulled into the dataset when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 01:19:49,874\tINFO worker.py:1242 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "find: ‘.git’: No such file or directory\n",
      "2023-02-06 01:19:50,207\tINFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.0.63.233:9031...\n",
      "2023-02-06 01:19:50,214\tINFO worker.py:1544 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale.com/api/v2/sessions/ses_vnmb5jgl4z6q98h61dx25rccju/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-02-06 01:19:50,219\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_ddd00605043f2a3b0bb38077ccc4d18c.zip' (0.35MiB) to Ray cluster...\n",
      "2023-02-06 01:19:50,225\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_ddd00605043f2a3b0bb38077ccc4d18c.zip'.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 28.13it/s]\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 858.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_SK</th>\n",
       "      <th>N_SALES</th>\n",
       "      <th>N_RETURNS</th>\n",
       "      <th>RETURN_PROBABILITY</th>\n",
       "      <th>CD_DEMO_SK</th>\n",
       "      <th>CD_GENDER</th>\n",
       "      <th>CD_MARITAL_STATUS</th>\n",
       "      <th>CD_EDUCATION_STATUS</th>\n",
       "      <th>CD_PURCHASE_ESTIMATE</th>\n",
       "      <th>CD_CREDIT_RATING</th>\n",
       "      <th>CD_DEP_COUNT</th>\n",
       "      <th>CD_DEP_EMPLOYED_COUNT</th>\n",
       "      <th>CD_DEP_COLLEGE_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12370725</td>\n",
       "      <td>393</td>\n",
       "      <td>57</td>\n",
       "      <td>0.145038</td>\n",
       "      <td>644828</td>\n",
       "      <td>F</td>\n",
       "      <td>W</td>\n",
       "      <td>Advanced Degree</td>\n",
       "      <td>6000</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12363711</td>\n",
       "      <td>351</td>\n",
       "      <td>23</td>\n",
       "      <td>0.065527</td>\n",
       "      <td>149925</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>Advanced Degree</td>\n",
       "      <td>1000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12365584</td>\n",
       "      <td>892</td>\n",
       "      <td>67</td>\n",
       "      <td>0.075112</td>\n",
       "      <td>949318</td>\n",
       "      <td>F</td>\n",
       "      <td>W</td>\n",
       "      <td>4 yr Degree</td>\n",
       "      <td>1000</td>\n",
       "      <td>High Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12366529</td>\n",
       "      <td>393</td>\n",
       "      <td>41</td>\n",
       "      <td>0.104326</td>\n",
       "      <td>1822552</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>2 yr Degree</td>\n",
       "      <td>8500</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12368417</td>\n",
       "      <td>372</td>\n",
       "      <td>32</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>182245</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>2 yr Degree</td>\n",
       "      <td>2000</td>\n",
       "      <td>High Risk</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12362216</td>\n",
       "      <td>362</td>\n",
       "      <td>36</td>\n",
       "      <td>0.099448</td>\n",
       "      <td>1891186</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>8500</td>\n",
       "      <td>High Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12361970</td>\n",
       "      <td>370</td>\n",
       "      <td>50</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>1144947</td>\n",
       "      <td>M</td>\n",
       "      <td>W</td>\n",
       "      <td>College</td>\n",
       "      <td>8500</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12364249</td>\n",
       "      <td>378</td>\n",
       "      <td>49</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>847827</td>\n",
       "      <td>M</td>\n",
       "      <td>W</td>\n",
       "      <td>Advanced Degree</td>\n",
       "      <td>6000</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12367535</td>\n",
       "      <td>383</td>\n",
       "      <td>43</td>\n",
       "      <td>0.112272</td>\n",
       "      <td>222398</td>\n",
       "      <td>F</td>\n",
       "      <td>W</td>\n",
       "      <td>Primary</td>\n",
       "      <td>9000</td>\n",
       "      <td>High Risk</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12364574</td>\n",
       "      <td>369</td>\n",
       "      <td>60</td>\n",
       "      <td>0.162602</td>\n",
       "      <td>422118</td>\n",
       "      <td>F</td>\n",
       "      <td>W</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>5500</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_SK  N_SALES  N_RETURNS RETURN_PROBABILITY  CD_DEMO_SK CD_GENDER  \\\n",
       "0     12370725      393         57           0.145038      644828         F   \n",
       "1     12363711      351         23           0.065527      149925         M   \n",
       "2     12365584      892         67           0.075112      949318         F   \n",
       "3     12366529      393         41           0.104326     1822552         F   \n",
       "4     12368417      372         32           0.086022      182245         M   \n",
       "5     12362216      362         36           0.099448     1891186         F   \n",
       "6     12361970      370         50           0.135135     1144947         M   \n",
       "7     12364249      378         49           0.129630      847827         M   \n",
       "8     12367535      383         43           0.112272      222398         F   \n",
       "9     12364574      369         60           0.162602      422118         F   \n",
       "\n",
       "  CD_MARITAL_STATUS CD_EDUCATION_STATUS  CD_PURCHASE_ESTIMATE  \\\n",
       "0                 W     Advanced Degree                  6000   \n",
       "1                 D     Advanced Degree                  1000   \n",
       "2                 W         4 yr Degree                  1000   \n",
       "3                 M         2 yr Degree                  8500   \n",
       "4                 D         2 yr Degree                  2000   \n",
       "5                 D             Unknown                  8500   \n",
       "6                 W             College                  8500   \n",
       "7                 W     Advanced Degree                  6000   \n",
       "8                 W             Primary                  9000   \n",
       "9                 W           Secondary                  5500   \n",
       "\n",
       "  CD_CREDIT_RATING  CD_DEP_COUNT  CD_DEP_EMPLOYED_COUNT  CD_DEP_COLLEGE_COUNT  \n",
       "0             Good             3                      2                     2  \n",
       "1          Unknown             5                      3                     0  \n",
       "2        High Risk             1                      3                     3  \n",
       "3         Low Risk             3                      4                     6  \n",
       "4        High Risk             4                      4                     0  \n",
       "5        High Risk             1                      6                     6  \n",
       "6         Low Risk             1                      1                     4  \n",
       "7         Low Risk             4                      0                     3  \n",
       "8        High Risk             4                      5                     0  \n",
       "9         Low Risk             5                      3                     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data import read_snowflake\n",
    "ds = read_snowflake(connect_props, query=query)\n",
    "    \n",
    "ds.limit(10).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train\n",
    "Now that the data is read into a Ray dataset, we can use it to train or tune a LighGBM model. \n",
    "\n",
    "##### Prepare the data\n",
    "After reading the data, we need to do some simple manipulations to drop columns and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The size in bytes of the block must be known: (ObjectRef(00ffffffffffffffffffffffffffffffffffffff0400000005000000), BlockMetadata(num_rows=1209, size_bytes=None, schema=PandasBlockSchema(names=['CUSTOMER_SK', 'N_SALES', 'N_RETURNS', 'RETURN_PROBABILITY', 'CD_DEMO_SK', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_PURCHASE_ESTIMATE', 'CD_CREDIT_RATING', 'CD_DEP_COUNT', 'CD_DEP_EMPLOYED_COUNT', 'CD_DEP_COLLEGE_COUNT'], types=[dtype('int64'), dtype('int64'), dtype('int64'), dtype('O'), dtype('int64'), dtype('O'), dtype('O'), dtype('O'), dtype('int64'), dtype('O'), dtype('int64'), dtype('int64'), dtype('int64')]), input_files=[], exec_stats=None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m DROP_COLUMNS \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mN_SALES\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mN_RETURNS\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCD_DEMO_SK\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mfully_executed()\u001b[39m.\u001b[39mdrop_columns(DROP_COLUMNS)\u001b[39m.\u001b[39mrepartition(\u001b[39m100\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_dataset, valid_dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mtrain_test_split(test_size\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m)\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/dataset.py:3972\u001b[0m, in \u001b[0;36mDataset.fully_executed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3963\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfully_executed\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataset[T]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3964\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Force full evaluation of the blocks of this dataset.\u001b[39;00m\n\u001b[1;32m   3965\u001b[0m \n\u001b[1;32m   3966\u001b[0m \u001b[39m    This can be used to read all blocks into memory. By default, Datasets\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3970\u001b[0m \u001b[39m        A Dataset with all blocks fully materialized in memory.\u001b[39;00m\n\u001b[1;32m   3971\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3972\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plan\u001b[39m.\u001b[39;49mexecute(force_read\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/plan.py:532\u001b[0m, in \u001b[0;36mExecutionPlan.execute\u001b[0;34m(self, allow_clear_input_blocks, force_read)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_internal\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexecution\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy_compat\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    528\u001b[0m     execute_to_legacy_block_list,\n\u001b[1;32m    529\u001b[0m )\n\u001b[1;32m    531\u001b[0m executor \u001b[39m=\u001b[39m BulkExecutor(copy\u001b[39m.\u001b[39mdeepcopy(context\u001b[39m.\u001b[39mexecution_options))\n\u001b[0;32m--> 532\u001b[0m blocks \u001b[39m=\u001b[39m execute_to_legacy_block_list(\n\u001b[1;32m    533\u001b[0m     executor,\n\u001b[1;32m    534\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    535\u001b[0m     allow_clear_input_blocks\u001b[39m=\u001b[39;49mallow_clear_input_blocks,\n\u001b[1;32m    536\u001b[0m     dataset_uuid\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_uuid,\n\u001b[1;32m    537\u001b[0m )\n\u001b[1;32m    538\u001b[0m \u001b[39m# TODO(ekl) we shouldn't need to set this in the future once we move\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39m# to a fully lazy execution model, unless .cache() is used. The reason\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39m# we need it right now is since the user may iterate over a Dataset\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39m# multiple times after fully executing it once.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_by_consumer:\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:83\u001b[0m, in \u001b[0;36mexecute_to_legacy_block_list\u001b[0;34m(executor, plan, allow_clear_input_blocks, dataset_uuid)\u001b[0m\n\u001b[1;32m     81\u001b[0m     dag, stats \u001b[39m=\u001b[39m get_execution_plan(plan\u001b[39m.\u001b[39m_logical_plan)\u001b[39m.\u001b[39mdag, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     dag, stats \u001b[39m=\u001b[39m _to_operator_dag(plan, allow_clear_input_blocks)\n\u001b[1;32m     84\u001b[0m bundles \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mexecute(dag, initial_stats\u001b[39m=\u001b[39mstats)\n\u001b[1;32m     85\u001b[0m _set_stats_uuid_recursive(executor\u001b[39m.\u001b[39mget_stats(), dataset_uuid)\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:104\u001b[0m, in \u001b[0;36m_to_operator_dag\u001b[0;34m(plan, allow_clear_input_blocks)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     owns_blocks \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m operator \u001b[39m=\u001b[39m _blocks_to_input_buffer(blocks, owns_blocks)\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m stage \u001b[39min\u001b[39;00m stages:\n\u001b[1;32m    106\u001b[0m     operator \u001b[39m=\u001b[39m _stage_to_operator(stage, operator)\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:157\u001b[0m, in \u001b[0;36m_blocks_to_input_buffer\u001b[0;34m(blocks, owns_blocks)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m MapOperator\u001b[39m.\u001b[39mcreate(do_read, inputs, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDoRead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m _block_list_to_bundles(blocks, owns_blocks\u001b[39m=\u001b[39;49mowns_blocks)\n\u001b[1;32m    158\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m output:\n\u001b[1;32m    159\u001b[0m         \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m i\u001b[39m.\u001b[39mblocks:\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/legacy_compat.py:276\u001b[0m, in \u001b[0;36m_block_list_to_bundles\u001b[0;34m(blocks, owns_blocks)\u001b[0m\n\u001b[1;32m    273\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m    274\u001b[0m \u001b[39mfor\u001b[39;00m block, meta \u001b[39min\u001b[39;00m blocks\u001b[39m.\u001b[39miter_blocks_with_metadata():\n\u001b[1;32m    275\u001b[0m     output\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 276\u001b[0m         RefBundle(\n\u001b[1;32m    277\u001b[0m             [\n\u001b[1;32m    278\u001b[0m                 (\n\u001b[1;32m    279\u001b[0m                     block,\n\u001b[1;32m    280\u001b[0m                     meta,\n\u001b[1;32m    281\u001b[0m                 )\n\u001b[1;32m    282\u001b[0m             ],\n\u001b[1;32m    283\u001b[0m             owns_blocks\u001b[39m=\u001b[39;49mowns_blocks,\n\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m<string>:5\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, blocks, owns_blocks)\u001b[0m\n",
      "File \u001b[0;32m/mnt/cluster_storage/pypi/lib/python3.10/site-packages/ray/data/_internal/execution/interfaces.py:44\u001b[0m, in \u001b[0;36mRefBundle.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(b[\u001b[39m1\u001b[39m], BlockMetadata), b\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m b[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msize_bytes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe size in bytes of the block must be known: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(b)\n\u001b[1;32m     46\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The size in bytes of the block must be known: (ObjectRef(00ffffffffffffffffffffffffffffffffffffff0400000005000000), BlockMetadata(num_rows=1209, size_bytes=None, schema=PandasBlockSchema(names=['CUSTOMER_SK', 'N_SALES', 'N_RETURNS', 'RETURN_PROBABILITY', 'CD_DEMO_SK', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_PURCHASE_ESTIMATE', 'CD_CREDIT_RATING', 'CD_DEP_COUNT', 'CD_DEP_EMPLOYED_COUNT', 'CD_DEP_COLLEGE_COUNT'], types=[dtype('int64'), dtype('int64'), dtype('int64'), dtype('O'), dtype('int64'), dtype('O'), dtype('O'), dtype('O'), dtype('int64'), dtype('O'), dtype('int64'), dtype('int64'), dtype('int64')]), input_files=[], exec_stats=None))"
     ]
    }
   ],
   "source": [
    "DROP_COLUMNS = ['N_SALES', 'N_RETURNS', 'CD_DEMO_SK']\n",
    "\n",
    "ds = ds.fully_executed().drop_columns(DROP_COLUMNS).repartition(100)\n",
    "train_dataset, valid_dataset = ds.train_test_split(test_size=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create preprocessors\n",
    "In Ray Air, all trainers, tuners and predcitors allow for the addition of preprocessors. Preprocessors help to featurize data, by providing common operations like on-hot-encoding, categorizing, scaling, etc. For more on the available preprocessors, read the [RayAIR docs](https://docs.ray.io/en/latest/ray-air/package-ref.html#preprocessor). The code below will use a chain of pre-processors. The `BatchMapper` will drop the ID column so it wont be used when training. The `Categorizer` will categorize columns, and the `StandardScaler` will scale columns. All of the pre-processing logic only modifes the data as it is being passed into training algorithms, and the underlying dataset will remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import Chain, BatchMapper, Categorizer, StandardScaler\n",
    "\n",
    "ID_COLUMN = 'CUSTOMER_SK'\n",
    "CATEGORICAL_COLUMNS = ['CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING']\n",
    "SCALAR_COLUMNS = ['CD_PURCHASE_ESTIMATE', 'CD_DEP_COUNT', 'CD_DEP_EMPLOYED_COUNT', 'CD_DEP_COLLEGE_COUNT']\n",
    "\n",
    "# Scale some random columns, and categorify the categorical_column,\n",
    "# allowing LightGBM to use its built-in categorical feature support\n",
    "preprocessor = Chain(\n",
    "    BatchMapper(lambda df: df.drop(ID_COLUMN, axis=1)),\n",
    "    Categorizer(CATEGORICAL_COLUMNS), \n",
    "    StandardScaler(columns=SCALAR_COLUMNS)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure scaling\n",
    "Training requires compute infrastructure, and specifying what type is needed to optimize your training time and costs. When first beginning, it is best to start with a small dataset size and compute to get things working and then scale up data and compute together. Below we create a `ScalingConfig` that provides 10 workers for distributed trianing. This will likely keep training on a single instance. We also don't request GPU's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "scaling_config=ScalingConfig(num_workers=10, use_gpu=False),"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a trainer\n",
    "Now that we have everything required, we can create a trainer. In Ray AIR, the logic to create a trainer and fit it are very simliar. The main differences are in the parameters passed to the algorithm. This makes it easy to swap out algorithms. For example, swapping LightGBM for XGBoost, or even PyTorch tabular, will typically be just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightgbm import LightGBMTrainer\n",
    "\n",
    "TARGET_COLUMN = 'RETURN_PROBABILITY'\n",
    "\n",
    "# LightGBM specific params\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": [\"rmse\", \"mae\"],\n",
    "}\n",
    "\n",
    "trainer = LightGBMTrainer(\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=False),\n",
    "    label_column=TARGET_COLUMN,\n",
    "    params=params,\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    "    num_boost_round=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model\n",
    "Now that the trainer is defined, al that is required is to call fit to begin the training process. The `fit` method will return a results object that containes the model checkpoint as well as model training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score a model\n",
    "Once there is a trained model, we can use it to score data. The flow for training and scoring are similar in that data is staged in Snowflake and read into a Ray dataset with the connector. Once the data is read in, the previously created model checkpoint can be used to creat a batch predictor for scoring. Scored data can then be written back into Snowflake with the connector. \n",
    "\n",
    "The typical logical flow for a batch scoring in Snowflake with Ray AIR is the following:\n",
    "\n",
    "![Snowflake batch scoring](images/snowflake_score_with_air.png)\n",
    "\n",
    "#### Steps 1-2: Stage and read data\n",
    "Since the data has already been staged and loaded, we dont need any extra code to do that now. Typically, you will have a script for training, and a script for scoring that will be run independently. The staging and loading of data should be sperated into a shared script that can be used by each of these workflows.\n",
    "\n",
    "#### Step 3: Score the data\n",
    "The previously trained checkpoint can be used to create a predictor. This predictor will already contain the pre-processors used to train the model. All that is needed is to drop the target column before feeding it into the model to simulate a real dataset where we dont know the results.\n",
    "\n",
    "> Note: Typically model checkpoints will be stored in a model registry provided by Weights and Biases or MLFlow, or into an objects store like S3. Checkpoints are written and read using the [checkpoint API](https://docs.ray.io/en/latest/ray-air/package-ref.html#ray.air.checkpoint.Checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.lightgbm import LightGBMPredictor\n",
    "\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    result.checkpoint, LightGBMPredictor\n",
    ")\n",
    "\n",
    "test_dataset = valid_dataset.drop_columns(TARGET_COLUMN)\n",
    "predictions = predictor.predict(test_dataset, keep_columns=[ID_COLUMN])\n",
    "predictions.limit(10).to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Write data to Snowflake\n",
    "Now that we have the predictions we can write them into a Snowflake table. We need to first create a destination database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.datasource import SnowflakeConnector\n",
    "\n",
    "# get new connect proeprties for the new database\n",
    "write_connect_props = {\n",
    "    **connect_props, \n",
    "    'database':'RAY_SAMPLE', \n",
    "    'schema':'PUBLIC'\n",
    "}\n",
    "\n",
    "# create destination database\n",
    "with SnowflakeConnector(**write_connect_props) as con:\n",
    "    con.query(f'CREATE DATABASE IF NOT EXISTS RAY_SAMPLE')\n",
    "    \n",
    "\n",
    "# write the predictions\n",
    "ds.write_snowflake(\n",
    "    write_connect_props, \n",
    "    table='PREDICTIONS',\n",
    "    autocreate=True\n",
    ")\n",
    "\n",
    "# read the predictions back\n",
    "read_snowflake(\n",
    "    write_connect_props, \n",
    "    table='PREDICTIONS'\n",
    ").limit(3).to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
